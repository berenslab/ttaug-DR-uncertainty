{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of weight layers : 15\n",
      "Residual stack depths : 1111 \n",
      "Building the model graph...\n",
      "Instance shape : [512, 512, 3]\n",
      "Batch shape : [None, 512, 512, 3]\n",
      "Num. of classes : 5\n",
      "Data augmentation probability : 0.9\n",
      "Biased-coin threshold : 0.1\n",
      "conv1\n",
      "\t[7, 7], 64 /2\n",
      "conv2/1\n",
      "\t[1, 1], 64 /1\n",
      "\t[3, 3], 64 /1\n",
      "\t[1, 1], 128 /1\n",
      "conv3/1\n",
      "\t[1, 1], 128 /2\n",
      "\t[3, 3], 128 /1\n",
      "\t[1, 1], 256 /1\n",
      "conv4/1\n",
      "\t[1, 1], 256 /2\n",
      "\t[3, 3], 256 /1\n",
      "\t[1, 1], 512 /1\n",
      "conv5/1\n",
      "\t[1, 1], 512 /2\n",
      "\t[3, 3], 512 /1\n",
      "\t[1, 1], 1024 /1\n",
      "fc1\n",
      "\t[2048, 512]\n",
      "Regularization type : l1\n",
      "logits\n",
      "\t[512, 5]\n"
     ]
    }
   ],
   "source": [
    "## TRAINING ONLY\n",
    "#######################################################################################\n",
    "import sys\n",
    "sys.path.insert(0, '../') # go up 1 level to include the project root in the search path.\n",
    "\n",
    "from models.MyResNet_Prefetcher import MyResNetPrefetcher\n",
    "from utils.Reader import AdvancedReader\n",
    "from utils.MyTimer import MyTimer\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('bmh')\n",
    "\n",
    "train_source = '/tmp/kaggle_dr_data/train_JF_BG_512/' \n",
    "val_source= '/tmp/kaggle_dr_data/test_JF_BG_512/' \n",
    "test_source = val_source \n",
    "\n",
    "train_csv_file = '/tmp/kaggle_dr_data/trainLabels.csv' # '/tmp/kaggle_dr_data/trainLabels.csv'\n",
    "solution_csv_file = '/tmp/kaggle_dr_data/retinopathy_solution.csv' # '/tmp/kaggle_dr_data/retinopathy_solution.csv'\n",
    "\n",
    "# Save results on GPFS01!\n",
    "# You may wish to change this to your local folder.\n",
    "RESULTS_DIR = '/gpfs01/berens/user/mayhan/Documents/MyPy/GitRepos/ttaug-DR-uncertainty/results/'\n",
    "\n",
    "network_config = dict([('instance_shape', [512, 512, 3]),\n",
    "                       ('num_classes', 5),\n",
    "                       ('conv_depths', [1, 1, 1, 1]),\n",
    "#                        ('num_filters', [[64, 64, 256], [128, 128, 512], [256, 256, 1024], [512, 512, 2048]]),\n",
    "                       ('num_filters', [[64, 64, 128], [128, 128, 256], [256, 256, 512], [512, 512, 1024]]),\n",
    "                       ('fc_depths', [512]),\n",
    "                       ('lambda', 0.1),\n",
    "                       ('lr', 0.005), \n",
    "                       ('momentum_max', 0.9),\n",
    "                       ('decay_steps', 10000),\n",
    "                       ('decay_rate', 0.8), \n",
    "                       ('data_aug', True),\n",
    "                       ('data_aug_prob', 0.9),\n",
    "                       ('max_iter', 500000),\n",
    "                       ('oversampling_limit', 0.1),\n",
    "                       ('batch_size', 23), # ResNet50: Max batch sizes allowed by BatchNorm and BatchReNorm are 14 and 8, respectively.\n",
    "                       ('val_step', 5000),\n",
    "                       ('resurrection_step', 25000), \n",
    "                       ('quick_dirty_val', False),\n",
    "                       ('T', 0), # To be set later on during Test-time augmentation with various values, {4,8,16...}\n",
    "                       ('dataset_buffer_size', 1000) # times minibatch size effectively\n",
    "                      ])\n",
    "\n",
    "model = MyResNetPrefetcher(network_config=network_config, name='MyResNet4DR_')\n",
    "model.build()\n",
    "model.initialize()\n",
    "with MyTimer('Timer'):\n",
    "    model.train(train_source=train_source, train_csv_file=train_csv_file,\n",
    "                val_source=val_source, solution_csv_file=solution_csv_file)\n",
    "model.finalize() \n",
    "\n",
    "diagnostics2save = model.diagnostics\n",
    "\n",
    "\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_DIAG.pkl'\n",
    "with open(result_file_name, 'wb') as filehandler:\n",
    "    pickle.dump(diagnostics2save, filehandler, protocol=4)\n",
    "\n",
    "# Clear some memory\n",
    "del diagnostics2save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING CURVES and VALIDATION PERFORMANCE ACROSS TRAINING\n",
    "#######################################################################################\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    diagnostics = pickle.load(filehandler)\n",
    "    \n",
    "    data1 = diagnostics['avg_losses']\n",
    "    losses = diagnostics['avg_losses']\n",
    "    tail_start = int(0.01*len(diagnostics['avg_losses']))\n",
    "    print('Tail start : %g' % tail_start)\n",
    "    data2 = losses[tail_start:]\n",
    "    \n",
    "    fig, ax1 = plt.subplots()    \n",
    "    color = 'tab:red'\n",
    "    ax1.set_xlabel('iteration')\n",
    "    ax1.set_ylabel('Avg. minibatch loss', color=color)\n",
    "    ax1.plot(range(0, len(diagnostics['avg_losses'])), data1, color=color, linestyle='--')\n",
    "    ax1.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    ax2 = ax1.twinx()  # instantiate a second axis that shares the same x-axis\n",
    "    color = 'tab:blue'\n",
    "    ax2.set_ylabel('Avg. minibatch loss (zoomed in)', color=color)  # we already handled the x-label with ax1\n",
    "    ax2.plot(range(tail_start, len(diagnostics['avg_losses'])), data2, color=color, linestyle=':')\n",
    "    ax2.tick_params(axis='y', labelcolor=color)\n",
    "    \n",
    "    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "    plt.show()          \n",
    "    \n",
    "    \n",
    "    # Validation performance across iterations\n",
    "    plt.figure()\n",
    "    \n",
    "    x = np.multiply(model.network_config['val_step'], list(range(0, len(diagnostics['val_roc1']))))\n",
    "    y = diagnostics['val_roc1']\n",
    "    plt.plot(x, y, color='m', linestyle='--', label='onset 1')\n",
    "    plt.plot(x, 0.889 * np.ones(shape=(len(y),1),dtype=np.float32), color='k', linestyle='--', label='Leibig et al., onset 1')\n",
    "    \n",
    "    x = np.multiply(model.network_config['val_step'], list(range(0, len(diagnostics['val_roc2']))))\n",
    "    y = diagnostics['val_roc2']\n",
    "    plt.plot(x, y, color='c', linestyle='-.', label='onset 2')\n",
    "    plt.plot(x, 0.927 * np.ones(shape=(len(y),1),dtype=np.float32), color='k', linestyle='-.', label='Leibig et al., onset 2')\n",
    "    \n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"ROC-AUC\")\n",
    "    #plt.title(\"ResNet50\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Clear some memory\n",
    "del diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SINGLE PREDICTION RESULTS\n",
    "#######################################################################################\n",
    "\n",
    "model.restore()\n",
    "\n",
    "print('=======================================================\\nEvaluating the performance on TRAINING set')\n",
    "# dr = AdvancedReader(source=train_source,\n",
    "#                     csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/trainLabels.csv', \n",
    "#                     mode = 'valtest' # valtest to read all from the training set\n",
    "#                    )\n",
    "labels_1hot_tr, predictions_1hot_tr, _, _, _, features_tr, logits_tr = model.inference(source=train_source,\n",
    "                                                                                       csv_file=train_csv_file,\n",
    "                                                                                       mode='valtest'\n",
    "                                                                                      )\n",
    "\n",
    "print('=======================================================\\nEvaluating the performance on VALIDATION set')\n",
    "# dr = AdvancedReader(source=test_source,\n",
    "#                     csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/retinopathy_solution.csv', \n",
    "#                     mode = 'val'\n",
    "#                    )\n",
    "labels_1hot_val, predictions_1hot_val, _, _, _, features_val, logits_val = model.inference(source=val_source,\n",
    "                                                                                           csv_file=solution_csv_file,\n",
    "                                                                                           mode='val'\n",
    "                                                                                          )\n",
    "\n",
    "print('=======================================================\\nEvaluating the performance on TEST set')\n",
    "# dr = AdvancedReader(source=test_source,\n",
    "#                     csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/retinopathy_solution.csv', \n",
    "#                     mode = 'test'\n",
    "#                    )\n",
    "labels_1hot_te, predictions_1hot_te, _, _, _, features_te, logits_te = model.inference(source=test_source, \n",
    "                                                                                       csv_file=solution_csv_file,\n",
    "                                                                                       mode='test'\n",
    "                                                                                      )\n",
    "\n",
    "#### Now, save the results\n",
    "result = {}\n",
    "result['train_labels_1hot'] = labels_1hot_tr\n",
    "result['val_labels_1hot'] = labels_1hot_val\n",
    "result['test_labels_1hot'] = labels_1hot_te\n",
    "result['train_pred_1hot'] = predictions_1hot_tr\n",
    "result['val_pred_1hot'] = predictions_1hot_val\n",
    "result['test_pred_1hot'] = predictions_1hot_te\n",
    "result['train_features'] = features_tr\n",
    "result['val_features'] = features_val\n",
    "result['test_features'] = features_te\n",
    "result['train_logits'] = logits_tr\n",
    "result['val_logits'] = logits_val\n",
    "result['test_logits'] = logits_te\n",
    "\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_SINGpred.pkl'\n",
    "with open(result_file_name, 'wb') as filehandler:\n",
    "    pickle.dump(result, filehandler, protocol=4)\n",
    "\n",
    "print('============\\nComparing the performance with Christians VGG-like network')\n",
    "# dr = AdvancedReader(source=test_source,\n",
    "#                     csv_file='/gpfs01/berens/user/mayhan/kaggle_dr_data/retinopathy_solution.csv', \n",
    "#                     mode = 'test'\n",
    "#                    )\n",
    "labels_1hot_te, predictions_1hot_te, _, _, _, features_te, logits_te = model.inference(source=test_source, \n",
    "                                                                                       csv_file=solution_csv_file,\n",
    "                                                                                       mode='valtest'\n",
    "                                                                                      )\n",
    "model.finalize()\n",
    "\n",
    "# Clear some memory\n",
    "del labels_1hot_tr, labels_1hot_val, labels_1hot_te\n",
    "del predictions_1hot_tr, predictions_1hot_val, predictions_1hot_te\n",
    "del features_tr, features_val, features_te\n",
    "del logits_tr, logits_val, logits_te\n",
    "del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## PLOTS FOR SINGLE PREDICTIONS\n",
    "############################################################################################\n",
    "from itertools import cycle\n",
    "import scipy.stats as stats\n",
    "\n",
    "def plot_roc_curve(labels, scores, linestyle, title, legend_prefix):\n",
    "    \n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    assert labels.shape[0] == scores.shape[0] and labels.shape[1] == scores.shape[1]\n",
    "\n",
    "    n_classes = labels.shape[1]\n",
    "    lw = 2\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels[:, i], scores[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])   \n",
    "\n",
    "    # Now plot the ROC-curves for classes\n",
    "    colors = cycle(['red', 'green', 'blue', 'deepskyblue', 'blueviolet'])\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr[i], tpr[i], color=color, lw=lw, ls=linestyle, dash_joinstyle='bevel', dash_capstyle='butt', \n",
    "                 label=legend_prefix + ' Class {0} (area = {1:0.5f})' ''.format(i, roc_auc[i]))\n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "\n",
    "def plot_roc_curves_for_all(result, title='Receiver Operating Characteristics'):\n",
    "    \"\"\"Inputs are in 1-hot or 1-vs-all format: Shape of [numOfExamples, numOfClasses]\n",
    "    The function plots the ROC curves for each binary classification scenario.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels_1hot_tr  = result['train_labels_1hot']\n",
    "    labels_1hot_val = result['val_labels_1hot']\n",
    "    labels_1hot_te = result['test_labels_1hot']\n",
    "    #labels_1hot_valte = result['valtest_labels_1hot']\n",
    "    predictions_1hot_tr = result['train_pred_1hot']\n",
    "    predictions_1hot_val = result['val_pred_1hot']\n",
    "    predictions_1hot_te = result['test_pred_1hot']\n",
    "    #predictions_1hot_valte = result['valtest_pred_1hot']\n",
    "    \n",
    "    plot_roc_curve(labels=labels_1hot_tr, scores=predictions_1hot_tr, linestyle=':', title=title, legend_prefix='Train, ')\n",
    "    plot_roc_curve(labels=labels_1hot_val, scores=predictions_1hot_val, linestyle='-.', title=title, legend_prefix='Val., ')\n",
    "    plot_roc_curve(labels=labels_1hot_te, scores=predictions_1hot_te, linestyle='--', title=title, legend_prefix='Test, ')\n",
    "    #plot_roc_curve(labels=labels_1hot_valte, scores=predictions_1hot_valte, linestyle='--', title=title, legend_prefix='ValTest, ')    \n",
    "    \n",
    "    leg = plt.legend(bbox_to_anchor=(1., 0.5, 0.625, 0.), loc='center right', ncol=1, mode=\"expand\", shadow=True, fancybox=True)\n",
    "    leg.get_frame().set_alpha(0.9)\n",
    "    plt.show()\n",
    "\n",
    "# Now, read the SINGLE PRED. results from file and plot\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    result = pickle.load(filehandler)\n",
    "    \n",
    "    # ROC curves for train,val and test data combined\n",
    "    plt.figure()\n",
    "    plot_roc_curves_for_all(result, '')\n",
    "\n",
    "# Clear some memory\n",
    "del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "### Test-time data augmentation for predictive uncertainty estimation\n",
    "### via GENERATOR mechanism\n",
    "##################################################\n",
    "\n",
    "T_values = [4, 8, 16, 32, 64, 128]\n",
    "for T in T_values:\n",
    "    print('T = %g' % T)\n",
    "    \n",
    "    # model = MyResNet(network_config=network_config, name='MyResNet51_')\n",
    "    model.restore()  # Restoring from meta-graph information does not permit new nodes in the graph \n",
    "    model.network_config['T'] = T\n",
    "    \n",
    "    print('=======================================================\\nEvaluating the performance on TRAINING set')\n",
    "    labels_1hot_tr, predictions_1hot_tr_ttaug, features_tr_ttaug, logits_tr_ttaug = model.inference_ttaug(source=train_source,\n",
    "                                                                                                          csv_file=train_csv_file,\n",
    "                                                                                                          mode='valtest'\n",
    "                                                                                                         )\n",
    "    \n",
    "    print('=======================================================\\nEvaluating the performance on VALIDATION set')\n",
    "    labels_1hot_val, predictions_1hot_val_ttaug, features_val_ttaug, logits_val_ttaug = model.inference_ttaug(source=val_source,\n",
    "                                                                                                        csv_file=solution_csv_file,\n",
    "                                                                                                        mode='val'\n",
    "                                                                                                       )\n",
    "    \n",
    "    print('=======================================================\\nEvaluating the performance on TEST set')\n",
    "    labels_1hot_te, predictions_1hot_te_ttaug, features_te_ttaug, logits_te_ttaug = model.inference_ttaug(source=test_source, \n",
    "                                                                                                          csv_file=solution_csv_file,\n",
    "                                                                                                          mode='test'\n",
    "                                                                                                         )\n",
    "    model.finalize()\n",
    "        \n",
    "    #### Now, save the results\n",
    "    result_ttaug = {}\n",
    "    result_ttaug['train_labels_1hot'] = labels_1hot_tr\n",
    "    result_ttaug['val_labels_1hot'] = labels_1hot_val\n",
    "    result_ttaug['test_labels_1hot'] = labels_1hot_te\n",
    "    result_ttaug['train_pred_1hot'] = predictions_1hot_tr_ttaug\n",
    "    result_ttaug['val_pred_1hot'] = predictions_1hot_val_ttaug\n",
    "    result_ttaug['test_pred_1hot'] = predictions_1hot_te_ttaug\n",
    "    result_ttaug['train_logits'] = logits_tr_ttaug\n",
    "    result_ttaug['val_logits'] = logits_val_ttaug\n",
    "    result_ttaug['test_logits'] = logits_te_ttaug\n",
    "    ## FEATURES TO BE SAVED SEPARATELY due to memory issues with T=128\n",
    "    # This can be further improved by re-arranging the order of operations. However, for now, go with the flow!!!\n",
    "#     result_ttaug['train_features'] = features_tr_ttaug\n",
    "#     result_ttaug['val_features'] = features_val_ttaug\n",
    "#     result_ttaug['test_features'] = features_te_ttaug\n",
    "        \n",
    "    result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '.pkl'\n",
    "    with open(result_file_name, 'wb') as filehandler:\n",
    "        pickle.dump(result_ttaug, filehandler, protocol=4)\n",
    "    \n",
    "    # Clear some memory\n",
    "    del labels_1hot_tr, labels_1hot_val, labels_1hot_te\n",
    "    del predictions_1hot_tr_ttaug, predictions_1hot_val_ttaug, predictions_1hot_te_ttaug\n",
    "    del logits_tr_ttaug, logits_val_ttaug, logits_te_ttaug\n",
    "    del result_ttaug\n",
    "    \n",
    "    # Now, save the FEATURES individually.\n",
    "#     print('Saving FEAT VAL')\n",
    "    result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '_FEATURES_val' + '.pkl'\n",
    "    with open(result_file_name, 'wb') as filehandler:\n",
    "        pickle.dump(features_val_ttaug, filehandler, protocol=4)\n",
    "    del features_val_ttaug\n",
    "    \n",
    "#     print('Saving FEAT TR')\n",
    "    result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '_FEATURES_train' + '.pkl'\n",
    "    with open(result_file_name, 'wb') as filehandler:\n",
    "        pickle.dump(features_tr_ttaug, filehandler, protocol=4)\n",
    "    del features_tr_ttaug    \n",
    "    \n",
    "#     print('Saving FEAT TE')\n",
    "    result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '_FEATURES_test' + '.pkl'\n",
    "    with open(result_file_name, 'wb') as filehandler:\n",
    "        pickle.dump(features_te_ttaug, filehandler, protocol=4)\n",
    "    del features_te_ttaug"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
