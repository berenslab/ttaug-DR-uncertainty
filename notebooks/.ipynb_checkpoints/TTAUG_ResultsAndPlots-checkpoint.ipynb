{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../') # go up 1 level to include the project root in the search path.\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set(style=\"ticks\")\n",
    "sns.set_context(\"paper\")\n",
    "# sns.set_palette(\"colorblind\") # muted, deep\n",
    "sns.set_palette(\"Paired\") # paired, cubehelix, husl\n",
    "# sns.set_palette(\"coolwarm\") # BrBG, RdBu_r, coolwarm\n",
    "# flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]\n",
    "# sns.set_palette(flatui)\n",
    "\n",
    "from models.MyResNet_Prefetcher import MyResNetPrefetcher\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "T_values = [4, 8, 16, 32, 64, 128]\n",
    "\n",
    "# Here, The network config is used to specify a model/results file w.r.t. a configuration.\n",
    "network_config = dict([('instance_shape', [512, 512, 3]),\n",
    "                       ('num_classes', 5),\n",
    "                       ('conv_depths', [1, 1, 1, 1]),\n",
    "#                        ('num_filters', [[64, 64, 256], [128, 128, 512], [256, 256, 1024], [512, 512, 2048]]),\n",
    "                       ('num_filters', [[64, 64, 128], [128, 128, 256], [256, 256, 512], [512, 512, 1024]]),\n",
    "                       ('fc_depths', [512]),\n",
    "                       ('lambda', 0.00001),\n",
    "                       ('lr', 0.003), \n",
    "                       ('momentum_max', 0.9),\n",
    "                       ('decay_steps', 10000),\n",
    "                       ('decay_rate', 0.8), \n",
    "                       ('data_aug', True),\n",
    "                       ('data_aug_prob', 0.9),\n",
    "                       ('max_iter', 2000),\n",
    "                       ('oversampling_limit', 0.1),\n",
    "                       ('batch_size', 23), # ResNet50: Max batch sizes allowed by BatchNorm and BatchReNorm are 14 and 8, respectively.\n",
    "                       ('val_step', 200),\n",
    "                       ('resurrection_step', 25000), \n",
    "                       ('quick_dirty_val', True),\n",
    "                       ('T', 0), # To be set later on during Test-time augmentation with various values, {4,8,16...}\n",
    "                       ('dataset_buffer_size', 500) # times minibatch size effectively\n",
    "                      ])\n",
    "\n",
    "# Now, set the file name for the results\n",
    "RESULTS_DIR = '/gpfs01/berens/user/mayhan/Documents/MyPy/GitRepos/ttaug-DR-uncertainty/results/'\n",
    "model = MyResNetPrefetcher(network_config=network_config, name='ResNet4GitHub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## TRAINING CURVES and VALIDATION PERFORMANCE ACROSS TRAINING\n",
    "#######################################################################################\n",
    "# Diagnostics file\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_DIAG.pkl'\n",
    "\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    diagnostics = pickle.load(filehandler)\n",
    "############################################\n",
    "df_loss = pd.DataFrame()\n",
    "df_loss['loss'] = np.concatenate([np.reshape(diagnostics['losses'], newshape=(len(diagnostics['losses']),)), \n",
    "                                  np.reshape(diagnostics['avg_losses'], newshape=(len(diagnostics['avg_losses']),))\n",
    "                                 ],\n",
    "                                 axis=0\n",
    "                                )\n",
    "df_loss['iteration'] = np.concatenate([np.reshape(range(0, len(diagnostics['losses'])), newshape=(len(diagnostics['losses']),)), \n",
    "                                       np.reshape(range(0, len(diagnostics['avg_losses'])), newshape=(len(diagnostics['avg_losses']),)),\n",
    "                                      ],\n",
    "                                      axis=0\n",
    "                                     )\n",
    "df_loss['Type'] = np.concatenate([np.reshape((('minibatch loss',) * len(diagnostics['losses'])), newshape=(len(diagnostics['losses']),)), \n",
    "                                  np.reshape((('avg. minibatch loss',) * len(diagnostics['avg_losses'])), newshape=(len(diagnostics['avg_losses']),))\n",
    "                                 ],\n",
    "                                 axis=0\n",
    "                                )\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7.5))\n",
    "ax1 = fig.add_subplot(1, 2, 1) \n",
    "ax1 = sns.lineplot(x='iteration', y='loss', hue='Type', data=df_loss, ax=ax1)\n",
    "sns.despine()\n",
    "\n",
    "############################################\n",
    "df_roc = pd.DataFrame()\n",
    "df_roc['ROC-AUC'] = np.concatenate([np.reshape(diagnostics['val_roc1'], newshape=(len(diagnostics['val_roc1']),)), \n",
    "                                    np.reshape(diagnostics['val_roc2'], newshape=(len(diagnostics['val_roc2']),)),\n",
    "                                    0.889 * np.ones(shape=(len(diagnostics['val_roc1']),)), \n",
    "                                    0.927 * np.ones(shape=(len(diagnostics['val_roc2']),))\n",
    "                                   ],\n",
    "                                   axis=0\n",
    "                                  )\n",
    "df_roc['iteration'] = np.concatenate([np.reshape(np.multiply(model.network_config['val_step'], list(range(0, len(diagnostics['val_roc1'])))), newshape=(len(diagnostics['val_roc1']),)), \n",
    "                                      np.reshape(np.multiply(model.network_config['val_step'], list(range(0, len(diagnostics['val_roc2'])))), newshape=(len(diagnostics['val_roc2']),)),\n",
    "                                      np.reshape(np.multiply(model.network_config['val_step'], list(range(0, len(diagnostics['val_roc1'])))), newshape=(len(diagnostics['val_roc1']),)), \n",
    "                                      np.reshape(np.multiply(model.network_config['val_step'], list(range(0, len(diagnostics['val_roc2'])))), newshape=(len(diagnostics['val_roc2']),))\n",
    "                                     ],\n",
    "                                     axis=0\n",
    "                                    )\n",
    "df_roc['Onset level'] = np.concatenate([np.reshape((('Mild DR',) * len(diagnostics['val_roc1'])), newshape=(len(diagnostics['val_roc1']),)), \n",
    "                                        np.reshape((('Moderate DR',) * len(diagnostics['val_roc2'])), newshape=(len(diagnostics['val_roc2']),)), \n",
    "                                        np.reshape((('Mild DR',) * len(diagnostics['val_roc1'])), newshape=(len(diagnostics['val_roc1']),)), \n",
    "                                        np.reshape((('Moderate DR',) * len(diagnostics['val_roc2'])), newshape=(len(diagnostics['val_roc2']),))\n",
    "                                       ],\n",
    "                                       axis=0\n",
    "                                      )\n",
    "df_roc['Method'] = np.concatenate([np.reshape((('ours',) * len(diagnostics['val_roc1'])), newshape=(len(diagnostics['val_roc1']),)), \n",
    "                                   np.reshape((('ours',) * len(diagnostics['val_roc2'])), newshape=(len(diagnostics['val_roc2']),)), \n",
    "                                   np.reshape((('Leibig et al.',) * len(diagnostics['val_roc1'])), newshape=(len(diagnostics['val_roc1']),)), \n",
    "                                   np.reshape((('Leibig et al.',) * len(diagnostics['val_roc2'])), newshape=(len(diagnostics['val_roc2']),))\n",
    "                                  ],\n",
    "                                  axis=0\n",
    "                                 )\n",
    "ax2 = fig.add_subplot(1, 2, 2) \n",
    "ax2 = sns.lineplot(x='iteration', y='ROC-AUC', hue='Onset level', style='Method', data=df_roc, ax=ax2)\n",
    "sns.despine()\n",
    "\n",
    "max_idx = np.argmax(diagnostics['val_roc1'])\n",
    "ax2.plot(model.network_config['val_step']*max_idx, diagnostics['val_roc1'][max_idx], color='lightcoral', \n",
    "         marker='D', markeredgecolor='k', markersize=3)\n",
    "max_idx = np.argmax(diagnostics['val_roc2'])\n",
    "ax2.plot(model.network_config['val_step']*max_idx, diagnostics['val_roc2'][max_idx], color='lightcoral', \n",
    "         marker='D', markeredgecolor='k', markersize=3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "del diagnostics, df_roc, df_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## PLOTS FOR SINGLE PREDICTIONS\n",
    "############################################################################################\n",
    "import scipy.stats as stats\n",
    "\n",
    "def plot_roc_curve(labels_1hot_tr, predictions_1hot_tr, \n",
    "                   labels_1hot_val, predictions_1hot_val, \n",
    "                   labels_1hot_te, predictions_1hot_te):\n",
    "    \n",
    "    legend_labels = np.array(['0: No DR', '1: Mild DR', '2: Moderate DR', '3: Severe DR', '4: Proliferative DR'])\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()    \n",
    "    \n",
    "    fpr_col = []\n",
    "    tpr_col = []\n",
    "    class_col = []\n",
    "    split_col = []\n",
    "    roc_auc_col = []\n",
    "    roc_auc_class_col = []\n",
    "    roc_auc_split_col = []\n",
    "    \n",
    "    # Training\n",
    "    split = 'Train'\n",
    "    for i in range(labels_1hot_tr.shape[1]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_1hot_tr[:, i], predictions_1hot_tr[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        fpr_col = np.concatenate([fpr_col, fpr[i]], axis=0)\n",
    "        tpr_col = np.concatenate([tpr_col, tpr[i]], axis=0)\n",
    "        class_col = np.concatenate([class_col, \n",
    "                                    np.reshape(((legend_labels[i],) * len(fpr[i])), newshape=(len(fpr[i]),))\n",
    "                                   ], axis=0\n",
    "                                  )\n",
    "        split_col = np.concatenate([split_col, \n",
    "                                    np.reshape(((split,) * len(fpr[i])), newshape=(len(fpr[i]),))\n",
    "                                   ], axis=0\n",
    "                                  )\n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(roc_auc[i], newshape=(1,))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        roc_auc_class_col = np.concatenate([roc_auc_class_col, \n",
    "                                            np.reshape(legend_labels[i], newshape=(1,))], \n",
    "                                           axis=0\n",
    "                                          )\n",
    "        roc_auc_split_col = np.concatenate([roc_auc_split_col, \n",
    "                                            np.reshape(split, newshape=(1,))], \n",
    "                                           axis=0\n",
    "                                          )\n",
    "    # Validation\n",
    "    split = 'Val.'\n",
    "    for i in range(labels_1hot_val.shape[1]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_1hot_val[:, i], predictions_1hot_val[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        fpr_col = np.concatenate([fpr_col, fpr[i]], axis=0)\n",
    "        tpr_col = np.concatenate([tpr_col, tpr[i]], axis=0)\n",
    "        class_col = np.concatenate([class_col, \n",
    "                                    np.reshape(((legend_labels[i],) * len(fpr[i])), newshape=(len(fpr[i]),))\n",
    "                                   ], axis=0\n",
    "                                  )\n",
    "        split_col = np.concatenate([split_col, \n",
    "                                    np.reshape(((split,) * len(fpr[i])), newshape=(len(fpr[i]),))\n",
    "                                   ], axis=0\n",
    "                                  )\n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(roc_auc[i], newshape=(1,))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        roc_auc_class_col = np.concatenate([roc_auc_class_col, \n",
    "                                            np.reshape(legend_labels[i], newshape=(1,))], \n",
    "                                           axis=0\n",
    "                                          )\n",
    "        roc_auc_split_col = np.concatenate([roc_auc_split_col, \n",
    "                                            np.reshape(split, newshape=(1,))], \n",
    "                                           axis=0\n",
    "                                          )\n",
    "    # Test\n",
    "    split = 'Test'\n",
    "    for i in range(labels_1hot_te.shape[1]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_1hot_te[:, i], predictions_1hot_te[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        fpr_col = np.concatenate([fpr_col, fpr[i]], axis=0)\n",
    "        tpr_col = np.concatenate([tpr_col, tpr[i]], axis=0)\n",
    "        class_col = np.concatenate([class_col, \n",
    "                                    np.reshape(((legend_labels[i],) * len(fpr[i])), newshape=(len(fpr[i]),))\n",
    "                                   ], axis=0\n",
    "                                  )\n",
    "        split_col = np.concatenate([split_col, \n",
    "                                    np.reshape(((split,) * len(fpr[i])), newshape=(len(fpr[i]),))\n",
    "                                   ], axis=0\n",
    "                                  )\n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(roc_auc[i], newshape=(1,))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        roc_auc_class_col = np.concatenate([roc_auc_class_col, \n",
    "                                            np.reshape(legend_labels[i], newshape=(1,))], \n",
    "                                           axis=0\n",
    "                                          )\n",
    "        roc_auc_split_col = np.concatenate([roc_auc_split_col, \n",
    "                                            np.reshape(split, newshape=(1,))], \n",
    "                                           axis=0\n",
    "                                          )        \n",
    "    \n",
    "    df_roc_multi = pd.DataFrame()\n",
    "    df_roc_multi['False Positive Rate'] = fpr_col\n",
    "    df_roc_multi['True Positive Rate'] = tpr_col\n",
    "    df_roc_multi['Class'] = class_col\n",
    "    df_roc_multi['Split'] = split_col\n",
    "    \n",
    "    df_roc_multi_summary = pd.DataFrame()\n",
    "    df_roc_multi_summary['ROC-AUC'] = roc_auc_col\n",
    "    df_roc_multi_summary['Class'] = roc_auc_class_col\n",
    "    df_roc_multi_summary['Split'] = roc_auc_split_col\n",
    "    \n",
    "    # ROC curves for train,val and test data combined\n",
    "#     plt.figure()\n",
    "    f = plt.figure(figsize=(15,7.5))\n",
    "    ax1 = f.add_subplot(1, 2, 1)\n",
    "    ax2 = f.add_subplot(1, 2, 2)\n",
    "    \n",
    "    ax1 = sns.lineplot(x='False Positive Rate', y='True Positive Rate', hue='Class', style='Split', ci=None, \n",
    "                       data=df_roc_multi, ax=ax1)\n",
    "    ax1.plot([0, 1], [0, 1], 'k-.')\n",
    "    sns.despine()\n",
    "    \n",
    "    ax2 = sns.pointplot(x='Class', y='ROC-AUC', hue='Split', ci=None, \n",
    "                       data=df_roc_multi_summary, ax=ax2)\n",
    "    sns.despine()\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curves_for_all(result, title='Receiver Operating Characteristics'):\n",
    "    \"\"\"Inputs are in 1-hot or 1-vs-all format: Shape of [numOfExamples, numOfClasses]\n",
    "    The function plots the ROC curves for each binary classification scenario.\n",
    "    \"\"\"\n",
    "    \n",
    "    labels_1hot_tr  = result['train_labels_1hot']\n",
    "    labels_1hot_val = result['val_labels_1hot']\n",
    "    labels_1hot_te = result['test_labels_1hot']\n",
    "    predictions_1hot_tr = result['train_pred_1hot']\n",
    "    predictions_1hot_val = result['val_pred_1hot']\n",
    "    predictions_1hot_te = result['test_pred_1hot']\n",
    "    \n",
    "    plot_roc_curve(labels_1hot_tr, predictions_1hot_tr, \n",
    "                   labels_1hot_val, predictions_1hot_val, \n",
    "                   labels_1hot_te, predictions_1hot_te)\n",
    "\n",
    "# Now, read the SINGLE PRED. results from file and plot\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_SINGpred.pkl'\n",
    "\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    result = pickle.load(filehandler)\n",
    "    \n",
    "# #     # ROC curves for train,val and test data combined\n",
    "#     plt.figure()\n",
    "    plot_roc_curves_for_all(result, '')\n",
    "\n",
    "del result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "### DISCRIMINATIVE Performance via Test-time data augmentation\n",
    "###############################################################\n",
    "\n",
    "def summary_ttaug_disc_helper(labels_1hot, predictions_1hot_ttaug):\n",
    "    # use the median of T predictions for the final class membership: Mx1x5 or Mx5\n",
    "    predictions_1hot_median = np.median(predictions_1hot_ttaug, axis=1)\n",
    "    # print('Shape of predictions_1hot_tr_median: ' + str(predictions_1hot_tr_median.shape) )\n",
    "    correct = np.equal(np.argmax(labels_1hot, axis=1), np.argmax(predictions_1hot_median, axis=1))\n",
    "    acc_median = np.mean(np.asarray(correct, dtype=np.float32))\n",
    "#     print('Median Accuracy (multi-class) : %.5f' % acc_median)    \n",
    "    predictions_1hot_mean = np.mean(predictions_1hot_ttaug, axis=1) \n",
    "    correct = np.equal(np.argmax(labels_1hot, axis=1), np.argmax(predictions_1hot_mean, axis=1))\n",
    "    acc_mean = np.mean(np.asarray(correct, dtype=np.float32))\n",
    "#     print('Mean Accuracy (multi-class) : %.5f' % acc_mean)\n",
    "        \n",
    "    onset_level = 1\n",
    "#     print('Onset level = %d' % onset_level)\n",
    "    labels_bin = np.greater_equal(np.argmax(labels_1hot, axis=1), onset_level)\n",
    "    pred_bin = np.sum(predictions_1hot_ttaug[:, :, onset_level:], axis=2) # MxTx1\n",
    "    pred_bin_median = np.median(pred_bin, axis=1) # Mx1x1  \n",
    "    fpr, tpr, _ = roc_curve(labels_bin, np.squeeze(pred_bin_median))\n",
    "    roc_auc_onset1_median = auc(fpr, tpr)\n",
    "#     print('With median pred., ROC-AUC: %.5f' % roc_auc_onset1_median)\n",
    "    pred_bin_mean = np.mean(pred_bin, axis=1) # Mx1x1  \n",
    "    fpr, tpr, _ = roc_curve(labels_bin, np.squeeze(pred_bin_mean))\n",
    "    roc_auc_onset1_mean = auc(fpr, tpr)\n",
    "#     print('With mean pred., ROC-AUC: %.5f' % roc_auc_onset1_mean)\n",
    "            \n",
    "    onset_level = 2\n",
    "#     print('Onset level = %d' % onset_level)\n",
    "    labels_bin = np.greater_equal(np.argmax(labels_1hot, axis=1), onset_level)\n",
    "    pred_bin = np.sum(predictions_1hot_ttaug[:, :, onset_level:], axis=2) # MxTx1\n",
    "    pred_bin_median = np.median(pred_bin, axis=1) # Mx1x1  \n",
    "    fpr, tpr, _ = roc_curve(labels_bin, np.squeeze(pred_bin_median))\n",
    "    roc_auc_onset2_median = auc(fpr, tpr)\n",
    "#     print('With median pred., ROC-AUC: %.5f' % roc_auc_onset2_median)\n",
    "    pred_bin_mean = np.mean(pred_bin, axis=1) # Mx1x1  \n",
    "    fpr, tpr, _ = roc_curve(labels_bin, np.squeeze(pred_bin_mean))\n",
    "    roc_auc_onset2_mean = auc(fpr, tpr)\n",
    "#     print('With mean pred., ROC-AUC: %.5f' % roc_auc_onset2_mean)\n",
    "    \n",
    "    return acc_median, acc_mean, roc_auc_onset1_median, roc_auc_onset1_mean, roc_auc_onset2_median, roc_auc_onset2_mean\n",
    "\n",
    "def summarize_ttaug_discriminative_performance(result):\n",
    "    labels_1hot_tr = result['train_labels_1hot'] # Mx5\n",
    "    labels_1hot_val = result['val_labels_1hot']\n",
    "    labels_1hot_te = result['test_labels_1hot']\n",
    "    predictions_1hot_tr_ttaug = result['train_pred_1hot'] # MxTx5\n",
    "    predictions_1hot_val_ttaug = result['val_pred_1hot']\n",
    "    predictions_1hot_te_ttaug = result['test_pred_1hot']\n",
    "    \n",
    "    # TRAINING\n",
    "#     print('Discriminative summary of training:')\n",
    "    tr_acc_median, tr_acc_mean, tr_roc_auc_onset1_median, tr_roc_auc_onset1_mean, tr_roc_auc_onset2_median, tr_roc_auc_onset2_mean = summary_ttaug_disc_helper(labels_1hot_tr, predictions_1hot_tr_ttaug)\n",
    "    # VALIDATION\n",
    "#     print('Discriminative summary of validation:')\n",
    "    val_acc_median, val_acc_mean, val_roc_auc_onset1_median, val_roc_auc_onset1_mean, val_roc_auc_onset2_median, val_roc_auc_onset2_mean = summary_ttaug_disc_helper(labels_1hot_val, predictions_1hot_val_ttaug)\n",
    "    # TEST\n",
    "#     print('Discriminative summary of test:')\n",
    "    te_acc_median, te_acc_mean, te_roc_auc_onset1_median, te_roc_auc_onset1_mean, te_roc_auc_onset2_median, te_roc_auc_onset2_mean = summary_ttaug_disc_helper(labels_1hot_te, predictions_1hot_te_ttaug)\n",
    "    \n",
    "    discriminative_summary = {}\n",
    "    discriminative_summary['tr_acc_median'] = tr_acc_median\n",
    "    discriminative_summary['tr_acc_mean'] = tr_acc_mean\n",
    "    discriminative_summary['tr_roc_auc_onset1_median'] = tr_roc_auc_onset1_median\n",
    "    discriminative_summary['tr_roc_auc_onset1_mean'] = tr_roc_auc_onset1_mean\n",
    "    discriminative_summary['tr_roc_auc_onset2_median'] = tr_roc_auc_onset2_median\n",
    "    discriminative_summary['tr_roc_auc_onset2_mean'] = tr_roc_auc_onset2_mean\n",
    "    \n",
    "    discriminative_summary['val_acc_median'] = val_acc_median\n",
    "    discriminative_summary['val_acc_mean'] = val_acc_mean\n",
    "    discriminative_summary['val_roc_auc_onset1_median'] = val_roc_auc_onset1_median\n",
    "    discriminative_summary['val_roc_auc_onset1_mean'] = val_roc_auc_onset1_mean\n",
    "    discriminative_summary['val_roc_auc_onset2_median'] = val_roc_auc_onset2_median\n",
    "    discriminative_summary['val_roc_auc_onset2_mean'] = val_roc_auc_onset2_mean\n",
    "    \n",
    "    discriminative_summary['te_acc_median'] = te_acc_median\n",
    "    discriminative_summary['te_acc_mean'] = te_acc_mean\n",
    "    discriminative_summary['te_roc_auc_onset1_median'] = te_roc_auc_onset1_median\n",
    "    discriminative_summary['te_roc_auc_onset1_mean'] = te_roc_auc_onset1_mean\n",
    "    discriminative_summary['te_roc_auc_onset2_median'] = te_roc_auc_onset2_median\n",
    "    discriminative_summary['te_roc_auc_onset2_mean'] = te_roc_auc_onset2_mean\n",
    "    \n",
    "    return discriminative_summary    \n",
    "    \n",
    "summaries = []\n",
    "for T in T_values:\n",
    "    print('T = %g' % T)\n",
    "    result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '.pkl'\n",
    "    \n",
    "    with open(result_file_name, 'rb') as filehandler:\n",
    "        result_ttaug = pickle.load(filehandler)\n",
    "        summaries.append(summarize_ttaug_discriminative_performance(result_ttaug))\n",
    "\n",
    "del result_ttaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Plot the discriminative performance summaries\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from collections import namedtuple\n",
    "\n",
    "markers = ['8','x','8','x','8','x','v','v','v']\n",
    "linestyles = ['-','--','-','--','-','--',':',':',':']\n",
    "order = ['4','8','16','32','64','128']\n",
    "\n",
    "############ MULTI-CLASS ACCURACY ############################\n",
    "\n",
    "# Firstly, determine the SINGLE PREDICTION baseline\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_SINGpred.pkl'\n",
    "\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    result = pickle.load(filehandler)\n",
    "\n",
    "labels_1hot_tr  = result['train_labels_1hot']\n",
    "labels_1hot_val = result['val_labels_1hot']\n",
    "labels_1hot_te = result['test_labels_1hot']\n",
    "predictions_1hot_tr = result['train_pred_1hot']\n",
    "predictions_1hot_val = result['val_pred_1hot']\n",
    "predictions_1hot_te = result['test_pred_1hot']\n",
    "\n",
    "# Accuracy baseline\n",
    "correct = np.equal(np.argmax(labels_1hot_tr, axis=-1), np.argmax(predictions_1hot_tr, axis=-1))\n",
    "baseline_acc_tr = np.mean(correct) * np.ones(shape=(len(T_values),1),dtype=np.float32)\n",
    "correct = np.equal(np.argmax(labels_1hot_val, axis=-1), np.argmax(predictions_1hot_val, axis=-1))\n",
    "baseline_acc_val = np.mean(correct) * np.ones(shape=(len(T_values),1),dtype=np.float32)\n",
    "correct = np.equal(np.argmax(labels_1hot_te, axis=-1), np.argmax(predictions_1hot_te, axis=-1))\n",
    "baseline_acc_te = np.mean(correct) * np.ones(shape=(len(T_values),1),dtype=np.float32)\n",
    "\n",
    "# Onset 1, ROC-AUC baseline\n",
    "onset_level = 1\n",
    "labels_bin = np.greater_equal(np.argmax(labels_1hot_tr, axis=1), onset_level)\n",
    "pred_bin = np.sum(predictions_1hot_tr[:, onset_level:], axis=1) # Mx1\n",
    "fpr, tpr, _ = roc_curve(labels_bin, pred_bin)\n",
    "baseline_roc_auc_onset1_tr = auc(fpr, tpr) * np.ones(shape=(len(T_values),1),dtype=np.float32)\n",
    "\n",
    "labels_bin = np.greater_equal(np.argmax(labels_1hot_val, axis=1), onset_level)\n",
    "pred_bin = np.sum(predictions_1hot_val[:, onset_level:], axis=1) # Mx1\n",
    "fpr, tpr, _ = roc_curve(labels_bin, pred_bin)\n",
    "baseline_roc_auc_onset1_val = auc(fpr, tpr) * np.ones(shape=(len(T_values),1),dtype=np.float32)\n",
    "\n",
    "labels_bin = np.greater_equal(np.argmax(labels_1hot_te, axis=1), onset_level)\n",
    "pred_bin = np.sum(predictions_1hot_te[:, onset_level:], axis=1) # Mx1\n",
    "fpr, tpr, _ = roc_curve(labels_bin, pred_bin)\n",
    "baseline_roc_auc_onset1_te = auc(fpr, tpr) * np.ones(shape=(len(T_values),1),dtype=np.float32)\n",
    "\n",
    "# Onset 2, ROC-AUC baseline\n",
    "onset_level = 2\n",
    "labels_bin = np.greater_equal(np.argmax(labels_1hot_tr, axis=1), onset_level)\n",
    "pred_bin = np.sum(predictions_1hot_tr[:, onset_level:], axis=1) # Mx1\n",
    "fpr, tpr, _ = roc_curve(labels_bin, pred_bin)\n",
    "baseline_roc_auc_onset2_tr = auc(fpr, tpr) * np.ones(shape=(len(T_values),1),dtype=np.float32)\n",
    "\n",
    "labels_bin = np.greater_equal(np.argmax(labels_1hot_val, axis=1), onset_level)\n",
    "pred_bin = np.sum(predictions_1hot_val[:, onset_level:], axis=1) # Mx1\n",
    "fpr, tpr, _ = roc_curve(labels_bin, pred_bin)\n",
    "baseline_roc_auc_onset2_val = auc(fpr, tpr) * np.ones(shape=(len(T_values),1),dtype=np.float32)\n",
    "\n",
    "labels_bin = np.greater_equal(np.argmax(labels_1hot_te, axis=1), onset_level)\n",
    "pred_bin = np.sum(predictions_1hot_te[:, onset_level:], axis=1) # Mx1\n",
    "fpr, tpr, _ = roc_curve(labels_bin, pred_bin)\n",
    "baseline_roc_auc_onset2_te = auc(fpr, tpr) * np.ones(shape=(len(T_values),1),dtype=np.float32)\n",
    "## end of baseline calculation\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "multi_acc_col = []\n",
    "pred_type_col = []\n",
    "T_col = []\n",
    "\n",
    "########### Multi-class Accuracy\n",
    "# Training, median\n",
    "pred_type = 'Train, median'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    multi_acc_col = np.concatenate([multi_acc_col,\n",
    "                                    np.reshape(summary['tr_acc_median'], newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Training, mean\n",
    "pred_type = 'Train, mean'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    multi_acc_col = np.concatenate([multi_acc_col,\n",
    "                                    np.reshape(summary['tr_acc_mean'], newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Validation, median\n",
    "pred_type = 'Val., median'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    multi_acc_col = np.concatenate([multi_acc_col,\n",
    "                                    np.reshape(summary['val_acc_median'], newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Validation, mean\n",
    "pred_type = 'Val., mean'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    multi_acc_col = np.concatenate([multi_acc_col,\n",
    "                                    np.reshape(summary['val_acc_mean'], newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Test, median\n",
    "pred_type = 'Test, median'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    multi_acc_col = np.concatenate([multi_acc_col,\n",
    "                                    np.reshape(summary['te_acc_median'], newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Test, mean\n",
    "pred_type = 'Test, mean'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    multi_acc_col = np.concatenate([multi_acc_col,\n",
    "                                    np.reshape(summary['te_acc_mean'], newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Baseline from single prediction\n",
    "pred_type = 'Train, single'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    \n",
    "    multi_acc_col = np.concatenate([multi_acc_col,\n",
    "                                    np.reshape(baseline_acc_tr[i], newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "pred_type = 'Val., single'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    \n",
    "    multi_acc_col = np.concatenate([multi_acc_col,\n",
    "                                    np.reshape(baseline_acc_val[i], newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "pred_type = 'Test, single'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    \n",
    "    multi_acc_col = np.concatenate([multi_acc_col,\n",
    "                                    np.reshape(baseline_acc_te[i], newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "\n",
    "# Plot multi-class accuracy across T values\n",
    "f = plt.figure(figsize=(7.5,7.5))\n",
    "ax1 = f.add_subplot(1, 1, 1)\n",
    "\n",
    "df_acc_multi_summary = pd.DataFrame()\n",
    "df_acc_multi_summary['Multi-class Accuracy'] = multi_acc_col\n",
    "df_acc_multi_summary['Prediction Type'] = pred_type_col\n",
    "df_acc_multi_summary['T'] = T_col\n",
    "\n",
    "ax1 = sns.pointplot(x='T', y='Multi-class Accuracy', hue='Prediction Type', ci=None, \n",
    "                    markers=markers, order=order,\n",
    "                    data=df_acc_multi_summary, ax=ax1)\n",
    "sns.despine()\n",
    "plt.show()\n",
    "###########End of Multi-class Accuracy################\n",
    "\n",
    "\n",
    "# Set up the figures for Onset 1 and 2 ROC-AUC plots\n",
    "# Plot multi-class accuracy across T values\n",
    "f, [ax1, ax2] = plt.subplots(1, 2, sharey='row', figsize=(15,7.5))\n",
    "# f = plt.figure(figsize=(15,7.5))\n",
    "# ax1 = f.add_subplot(1, 2, 1)\n",
    "# ax2 = f.add_subplot(1, 2, 2)\n",
    "\n",
    "############### ROC-AUC Onset 1 and 2 ################\n",
    "# Diagnostics file\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_DIAG.pkl'\n",
    "\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    diagnostics = pickle.load(filehandler) \n",
    "    max_roc1_idx = np.argmax(diagnostics['val_roc1'])\n",
    "\n",
    "#### Onset 1\n",
    "baseline = diagnostics['val_roc1'][max_roc1_idx] * np.ones(shape=(len(T_values),1),dtype=np.float32)\n",
    "\n",
    "roc_col = []\n",
    "pred_type_col = []\n",
    "T_col = []\n",
    "\n",
    "########### ROC-AUC Onset 1\n",
    "# Training, median\n",
    "pred_type = 'Train, median'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(summary['tr_roc_auc_onset1_median'], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Training, mean\n",
    "pred_type = 'Train, mean'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(summary['tr_roc_auc_onset1_mean'], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Validation, median\n",
    "pred_type = 'Val., median'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(summary['val_roc_auc_onset1_median'], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Validation, mean\n",
    "pred_type = 'Val., mean'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(summary['val_roc_auc_onset1_mean'], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Test, median\n",
    "pred_type = 'Test, median'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(summary['te_roc_auc_onset1_median'], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Test, mean\n",
    "pred_type = 'Test, mean'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(summary['te_roc_auc_onset1_mean'], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Baseline from single prediction\n",
    "pred_type = 'Train, single'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(baseline_roc_auc_onset1_tr[i], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "pred_type = 'Val., single'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(baseline_roc_auc_onset1_val[i], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "pred_type = 'Test, single'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(baseline_roc_auc_onset1_te[i], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "\n",
    "df_roc1_summary = pd.DataFrame()\n",
    "df_roc1_summary['ROC-AUC'] = roc_col\n",
    "df_roc1_summary['Prediction Type'] = pred_type_col\n",
    "df_roc1_summary['T'] = T_col\n",
    "# ax1 = sns.lineplot(x='T', y='ROC-AUC', hue='Split', style='Prediction Type', ci=None, \n",
    "#                    sort=False, markers=['o','d','s'], \n",
    "#                    data=df_roc1_summary, ax=ax1)\n",
    "ax1 = sns.pointplot(x='T', y='ROC-AUC', hue='Prediction Type', ci=None, markers=markers, order=order, \n",
    "                    data=df_roc1_summary, ax=ax1)\n",
    "sns.despine()\n",
    "\n",
    "\n",
    "#### Onset 2\n",
    "baseline = diagnostics['val_roc2'][max_roc1_idx] * np.ones(shape=(len(T_values),1),dtype=np.float32)\n",
    "\n",
    "roc_col = []\n",
    "pred_type_col = []\n",
    "T_col = []\n",
    "\n",
    "########### ROC-AUC Onset 2\n",
    "# Training, median\n",
    "pred_type = 'Train, median'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(summary['tr_roc_auc_onset2_median'], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Training, mean\n",
    "pred_type = 'Train, mean'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(summary['tr_roc_auc_onset2_mean'], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Validation, median\n",
    "pred_type = 'Val., median'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(summary['val_roc_auc_onset2_median'], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Validation, mean\n",
    "pred_type = 'Val., mean'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(summary['val_roc_auc_onset2_mean'], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Test, median\n",
    "pred_type = 'Test, median'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(summary['te_roc_auc_onset2_median'], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Test, mean\n",
    "pred_type = 'Test, mean'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    summary = summaries[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(summary['te_roc_auc_onset2_mean'], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "# Baseline from single prediction\n",
    "pred_type = 'Train, single'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(baseline_roc_auc_onset2_tr[i], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "pred_type = 'Val., single'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(baseline_roc_auc_onset2_val[i], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "pred_type = 'Test, single'\n",
    "for i in range(len(T_values)):\n",
    "    T = T_values[i]\n",
    "    roc_col = np.concatenate([roc_col,\n",
    "                              np.reshape(baseline_roc_auc_onset2_te[i], newshape=(1,))\n",
    "                             ], \n",
    "                             axis=0\n",
    "                            )\n",
    "    pred_type_col = np.concatenate([pred_type_col,\n",
    "                                    np.reshape(pred_type, newshape=(1,))\n",
    "                                   ], \n",
    "                                   axis=0\n",
    "                                  )\n",
    "    T_col = np.concatenate([T_col,\n",
    "                            np.reshape(str(T), newshape=(1,))\n",
    "                           ], \n",
    "                           axis=0\n",
    "                          )\n",
    "\n",
    "df_roc2_summary = pd.DataFrame()\n",
    "df_roc2_summary['ROC-AUC'] = roc_col\n",
    "df_roc2_summary['Prediction Type'] = pred_type_col\n",
    "df_roc2_summary['T'] = T_col\n",
    "ax2 = sns.pointplot(x='T', y='ROC-AUC', hue='Prediction Type', ci=None, markers=markers, order=order, \n",
    "                    data=df_roc2_summary, ax=ax2)\n",
    "sns.despine()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del df_acc_multi_summary, df_roc1_summary, df_roc2_summary, multi_acc_col, pred_type_col, T_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## PLOTS FOR TTAUG RESULTS: Multi-class ROC curves\n",
    "############################################################################################\n",
    "from itertools import cycle\n",
    "import scipy.stats as stats\n",
    "\n",
    "def normalize_softmax_from_ttaug(predictions_1hot):\n",
    "    return np.divide(predictions_1hot, np.sum(predictions_1hot, axis=-1, keepdims=True))\n",
    "\n",
    "def make_dataframe_for_rocauc(labels_1hot_tr, predictions_1hot_tr, \n",
    "                              labels_1hot_val, predictions_1hot_val, \n",
    "                              labels_1hot_te, predictions_1hot_te, scheme):\n",
    "    \n",
    "    legend_labels = np.array(['0: No DR', '1: Mild DR', '2: Moderate DR', '3: Severe DR', '4: Proliferative DR'])\n",
    "\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()    \n",
    "\n",
    "    roc_auc_col = []\n",
    "    roc_auc_class_col = []\n",
    "    roc_auc_split_col = []\n",
    "\n",
    "    # Training\n",
    "    split = 'Train, ' + str(scheme)\n",
    "    for i in range(labels_1hot_tr.shape[1]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_1hot_tr[:, i], predictions_1hot_tr[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(roc_auc[i], newshape=(1,))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        roc_auc_class_col = np.concatenate([roc_auc_class_col, \n",
    "                                            np.reshape(legend_labels[i], newshape=(1,))], \n",
    "                                           axis=0\n",
    "                                          )\n",
    "        roc_auc_split_col = np.concatenate([roc_auc_split_col, \n",
    "                                            np.reshape(split, newshape=(1,))], \n",
    "                                           axis=0\n",
    "                                          )\n",
    "    df_roc_multi_summary_tr = pd.DataFrame()\n",
    "    df_roc_multi_summary_tr['ROC-AUC'] = roc_auc_col\n",
    "    df_roc_multi_summary_tr['Class'] = roc_auc_class_col\n",
    "    df_roc_multi_summary_tr['Split'] = roc_auc_split_col\n",
    "    \n",
    "    roc_auc_col = []\n",
    "    roc_auc_class_col = []\n",
    "    roc_auc_split_col = []\n",
    "    \n",
    "    # Validation\n",
    "    split = 'Val., ' + str(scheme)\n",
    "    for i in range(labels_1hot_val.shape[1]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_1hot_val[:, i], predictions_1hot_val[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(roc_auc[i], newshape=(1,))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        roc_auc_class_col = np.concatenate([roc_auc_class_col, \n",
    "                                            np.reshape(legend_labels[i], newshape=(1,))], \n",
    "                                           axis=0\n",
    "                                          )\n",
    "        roc_auc_split_col = np.concatenate([roc_auc_split_col, \n",
    "                                            np.reshape(split, newshape=(1,))], \n",
    "                                           axis=0\n",
    "                                          )\n",
    "    df_roc_multi_summary_val = pd.DataFrame()\n",
    "    df_roc_multi_summary_val['ROC-AUC'] = roc_auc_col\n",
    "    df_roc_multi_summary_val['Class'] = roc_auc_class_col\n",
    "    df_roc_multi_summary_val['Split'] = roc_auc_split_col\n",
    "    \n",
    "    roc_auc_col = []\n",
    "    roc_auc_class_col = []\n",
    "    roc_auc_split_col = []\n",
    "    \n",
    "    # Test\n",
    "    split = 'Test, ' + str(scheme)\n",
    "    for i in range(labels_1hot_te.shape[1]):\n",
    "        fpr[i], tpr[i], _ = roc_curve(labels_1hot_te[:, i], predictions_1hot_te[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(roc_auc[i], newshape=(1,))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        roc_auc_class_col = np.concatenate([roc_auc_class_col, \n",
    "                                            np.reshape(legend_labels[i], newshape=(1,))], \n",
    "                                           axis=0\n",
    "                                          )\n",
    "        roc_auc_split_col = np.concatenate([roc_auc_split_col, \n",
    "                                            np.reshape(split, newshape=(1,))], \n",
    "                                           axis=0\n",
    "                                          )        \n",
    "    df_roc_multi_summary_te = pd.DataFrame()\n",
    "    df_roc_multi_summary_te['ROC-AUC'] = roc_auc_col\n",
    "    df_roc_multi_summary_te['Class'] = roc_auc_class_col\n",
    "    df_roc_multi_summary_te['Split'] = roc_auc_split_col\n",
    "#     df_roc_multi_summary['Scheme'] = np.reshape(((scheme,) * len(roc_auc_split_col)), \n",
    "#                                                 newshape=(len(roc_auc_split_col),))\n",
    "    \n",
    "    return df_roc_multi_summary_tr, df_roc_multi_summary_val, df_roc_multi_summary_te\n",
    "\n",
    "\n",
    "def plot_roc_curves_for_all_ttaug(result, title='Receiver Operating Characteristics', mode='mean'):\n",
    "    legend_labels = np.array(['0: No DR', '1: Mild DR', '2: Moderate DR', '3: Severe DR', '4: Proliferative DR'])\n",
    "        \n",
    "    labels_1hot_tr  = result['train_labels_1hot'] # MxC\n",
    "    labels_1hot_val = result['val_labels_1hot']\n",
    "    labels_1hot_te = result['test_labels_1hot']\n",
    "    predictions_1hot_tr = result['train_pred_1hot'] # MxTxC\n",
    "    predictions_1hot_val = result['val_pred_1hot']\n",
    "    predictions_1hot_te = result['test_pred_1hot']\n",
    "    \n",
    "    if mode=='mean':\n",
    "        predictions_1hot_tr = normalize_softmax_from_ttaug(np.mean(predictions_1hot_tr, axis=1))\n",
    "        predictions_1hot_val = normalize_softmax_from_ttaug(np.mean(predictions_1hot_val, axis=1))\n",
    "        predictions_1hot_te = normalize_softmax_from_ttaug(np.mean(predictions_1hot_te, axis=1))\n",
    "    else:   \n",
    "        predictions_1hot_tr= normalize_softmax_from_ttaug(np.median(predictions_1hot_tr, axis=1))\n",
    "        predictions_1hot_val = normalize_softmax_from_ttaug(np.median(predictions_1hot_val, axis=1))\n",
    "        predictions_1hot_te = normalize_softmax_from_ttaug(np.median(predictions_1hot_te, axis=1))\n",
    "    \n",
    "    plot_roc_curve(labels_1hot_tr, predictions_1hot_tr, \n",
    "                   labels_1hot_val, predictions_1hot_val, \n",
    "                   labels_1hot_te, predictions_1hot_te)\n",
    "            \n",
    "mode = 'mean'\n",
    "for T in T_values:\n",
    "    print('T = %g' % T)\n",
    "    result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '.pkl'\n",
    "    \n",
    "    with open(result_file_name, 'rb') as filehandler:\n",
    "        result_ttaug = pickle.load(filehandler)\n",
    "        plot_roc_curves_for_all_ttaug(result_ttaug, mode=mode)\n",
    "\n",
    "#########################################################################################################\n",
    "print('Now plotting the SINGLE vs TTAUG connected dot plots')\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_SINGpred.pkl'\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    result = pickle.load(filehandler)\n",
    "\n",
    "labels_1hot_tr  = result['train_labels_1hot']\n",
    "labels_1hot_val = result['val_labels_1hot']\n",
    "labels_1hot_te = result['test_labels_1hot']\n",
    "predictions_1hot_tr = result['train_pred_1hot']\n",
    "predictions_1hot_val = result['val_pred_1hot']\n",
    "predictions_1hot_te = result['test_pred_1hot']\n",
    "    \n",
    "df_roc_multi_summary_tr, df_roc_multi_summary_val, df_roc_multi_summary_te = make_dataframe_for_rocauc(labels_1hot_tr, predictions_1hot_tr, \n",
    "                                                                                                       labels_1hot_val, predictions_1hot_val, \n",
    "                                                                                                       labels_1hot_te, predictions_1hot_te, scheme='Sing. pred.')\n",
    "T = 128\n",
    "print('T = %g' % T)\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '.pkl'\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    result_ttaug = pickle.load(filehandler)\n",
    "\n",
    "labels_1hot_tr  = result_ttaug['train_labels_1hot'] # MxC\n",
    "labels_1hot_val = result_ttaug['val_labels_1hot']\n",
    "labels_1hot_te = result_ttaug['test_labels_1hot']\n",
    "predictions_1hot_tr = result_ttaug['train_pred_1hot'] # MxTxC\n",
    "predictions_1hot_val = result_ttaug['val_pred_1hot']\n",
    "predictions_1hot_te = result_ttaug['test_pred_1hot']\n",
    "\n",
    "if mode=='mean':\n",
    "    predictions_1hot_tr = normalize_softmax_from_ttaug(np.mean(predictions_1hot_tr, axis=1))\n",
    "    predictions_1hot_val = normalize_softmax_from_ttaug(np.mean(predictions_1hot_val, axis=1))\n",
    "    predictions_1hot_te = normalize_softmax_from_ttaug(np.mean(predictions_1hot_te, axis=1))\n",
    "else:   \n",
    "    predictions_1hot_tr= normalize_softmax_from_ttaug(np.median(predictions_1hot_tr, axis=1))\n",
    "    predictions_1hot_val = normalize_softmax_from_ttaug(np.median(predictions_1hot_val, axis=1))\n",
    "    predictions_1hot_te = normalize_softmax_from_ttaug(np.median(predictions_1hot_te, axis=1))\n",
    "\n",
    "df_roc_multi_summary_tr_ttaug, df_roc_multi_summary_val_ttaug, df_roc_multi_summary_te_ttaug = make_dataframe_for_rocauc(labels_1hot_tr, predictions_1hot_tr, \n",
    "                                                                                                                         labels_1hot_val, predictions_1hot_val, \n",
    "                                                                                                                         labels_1hot_te, predictions_1hot_te, scheme='TTAUG')\n",
    "df_2_comp = pd.concat([df_roc_multi_summary_tr, df_roc_multi_summary_tr_ttaug,\n",
    "                       df_roc_multi_summary_val, df_roc_multi_summary_val_ttaug,\n",
    "                       df_roc_multi_summary_te, df_roc_multi_summary_te_ttaug,])\n",
    "\n",
    "# Plot the comparison\n",
    "f = plt.figure(figsize=(7.5,7.5))\n",
    "ax1 = f.add_subplot(1, 1, 1)\n",
    "ax1 = sns.pointplot(x='Class', y='ROC-AUC', hue='Split', ci=None, \n",
    "                   data=df_2_comp, ax=ax1)\n",
    "sns.despine()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "del result_ttaug, result, df_2_comp, \n",
    "del df_roc_multi_summary_tr, df_roc_multi_summary_val, df_roc_multi_summary_te, df_roc_multi_summary_tr_ttaug, df_roc_multi_summary_val_ttaug, df_roc_multi_summary_te_ttaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Distribution of uncertainties across correct and misclassifications\n",
    "# def entropy(p, axis=-1, keepdims=False):\n",
    "#     return -np.sum(np.multiply(p, np.log(np.add(p,1e-6))), axis=axis, keepdims=keepdims)\n",
    "\n",
    "def entropy(p, axis=-1, keepdims=False):\n",
    "    # smoothing before entropy to avoid log 0s\n",
    "    p = np.add(p, 1e-6) # add a small constant to all values\n",
    "    p = np.divide(p, np.sum(p, axis=axis, keepdims=True)) # re-normalize the probabilities\n",
    "    return -np.sum(np.multiply(p, np.log(p)), axis=axis, keepdims=keepdims)\n",
    "\n",
    "def hist_uncertainty_binary_helper(labels_1hot, predictions_1hot_ttaug, num_bins, density=False, cumulative=False):\n",
    "    onset_levels = [1, 2]\n",
    "    bins = np.linspace(0., 1., num_bins)\n",
    "    \n",
    "    for onset_level in onset_levels:\n",
    "        print('Onset level: %d'  % onset_level)\n",
    "        f = plt.figure(figsize=(16,4))\n",
    "        \n",
    "        labels_bin = np.greater_equal(np.argmax(labels_1hot, axis=1), onset_level)\n",
    "        predictions_all_bin = np.sum(predictions_1hot_ttaug[:, :, onset_level:], axis=2) # MxTx5 --> MxT\n",
    "        \n",
    "        # Median and IQR\n",
    "        predictions_bin_median = np.greater_equal(np.median(predictions_all_bin, axis=1), 0.5)\n",
    "        correct = np.equal(labels_bin, predictions_bin_median)\n",
    "        acc_median = np.mean(np.asarray(correct, dtype=np.float32))\n",
    "        print('Median\\'s accuracy (multi-class) : %.5f' % acc_median)    \n",
    "        \n",
    "        ax1 = f.add_subplot(1, 4, 1)\n",
    "        uncertainty_est = stats.iqr(np.squeeze(predictions_all_bin[np.where(correct == True), :]), axis=1) # IQR from CxT matrix\n",
    "        sns.distplot(uncertainty_est, bins=bins, hist=True, kde=True, rug=False, fit=None, \n",
    "                     hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, \n",
    "                     color='g', vertical=False, norm_hist=False, \n",
    "                     axlabel='IQR', label='correct', ax=ax1)\n",
    "        sns.despine()\n",
    "    \n",
    "        uncertainty_est = stats.iqr(np.squeeze(predictions_all_bin[np.where(correct == False), :]), axis=1) # IQR from non-CxT matrix\n",
    "        sns.distplot(uncertainty_est, bins=bins, hist=True, kde=True, rug=False, fit=None, \n",
    "                     hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, \n",
    "                     color='r', vertical=False, norm_hist=False, \n",
    "                     axlabel='IQR', label='missed', ax=ax1)\n",
    "        sns.despine()\n",
    "        \n",
    "        # Entropy for median predictions\n",
    "        predictions_bin_score_median = np.median(predictions_all_bin, axis=1, keepdims=True)\n",
    "        softmax_median = np.concatenate((np.subtract(1.,predictions_bin_score_median), predictions_bin_score_median), axis=1) # Mx2\n",
    "        uncertainty_est_median_entropy = entropy(softmax_median) # ENT from Mx2 matrix\n",
    "        \n",
    "        ax3 = f.add_subplot(1, 4, 2)\n",
    "        uncertainty_est = uncertainty_est_median_entropy[np.where(correct == True)] # Entropy of CORRECTS\n",
    "        sns.distplot(uncertainty_est, bins=bins, hist=True, kde=True, rug=False, fit=None, \n",
    "                     hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, \n",
    "                     color='g', vertical=False, norm_hist=False, \n",
    "                     axlabel='ENTROPY w.r.t. median', label='correct', ax=ax3)\n",
    "        sns.despine()\n",
    "\n",
    "        uncertainty_est = uncertainty_est_median_entropy[np.where(correct == False)] # Entropy of MISSED\n",
    "        sns.distplot(uncertainty_est, bins=bins, hist=True, kde=True, rug=False, fit=None, \n",
    "                     hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, \n",
    "                     color='r', vertical=False, norm_hist=False, \n",
    "                     axlabel='ENTROPY w.r.t. median', label='missed', ax=ax3)\n",
    "        sns.despine()\n",
    "        \n",
    "        # Mean and STD\n",
    "        predictions_bin_mean = np.greater_equal(np.mean(predictions_all_bin, axis=1), 0.5)\n",
    "        correct = np.equal(labels_bin, predictions_bin_mean)\n",
    "        acc_mean = np.mean(np.asarray(correct, dtype=np.float32))\n",
    "        print('Mean\\'s accuracy (multi-class) : %.5f' % acc_mean)\n",
    "        \n",
    "        ax2 = f.add_subplot(1, 4, 3)\n",
    "        uncertainty_est = np.std(np.squeeze(predictions_all_bin[np.where(correct == True), :]), axis=1) # STD from CxT matrix\n",
    "        sns.distplot(uncertainty_est, bins=bins, hist=True, kde=True, rug=False, fit=None, \n",
    "                     hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, \n",
    "                     color='g', vertical=False, norm_hist=False, \n",
    "                     axlabel='STD', label='correct', ax=ax2)\n",
    "        sns.despine()\n",
    "    \n",
    "        uncertainty_est = np.std(np.squeeze(predictions_all_bin[np.where(correct == False), :]), axis=1) # STD from non-CxT matrix\n",
    "        sns.distplot(uncertainty_est, bins=bins, hist=True, kde=True, rug=False, fit=None, \n",
    "                     hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, \n",
    "                     color='r', vertical=False, norm_hist=False, \n",
    "                     axlabel='STD', label='missed', ax=ax2)\n",
    "        sns.despine()\n",
    "        \n",
    "        # Entropy for median predictions\n",
    "        predictions_bin_score_mean = np.mean(predictions_all_bin, axis=1, keepdims=True)\n",
    "        softmax_mean = np.concatenate((np.subtract(1.,predictions_bin_score_mean), predictions_bin_score_mean), axis=1) # Mx2\n",
    "        uncertainty_est_mean_entropy = entropy(softmax_mean) # ENT from Mx2 matrix\n",
    "        \n",
    "        ax4 = f.add_subplot(1, 4, 4)\n",
    "        uncertainty_est = uncertainty_est_mean_entropy[np.where(correct == True)] # Entropy of CORRECTS\n",
    "        sns.distplot(uncertainty_est, bins=bins, hist=True, kde=True, rug=False, fit=None, \n",
    "                     hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, \n",
    "                     color='g', vertical=False, norm_hist=False, \n",
    "                     axlabel='ENTROPY w.r.t. mean', label='correct', ax=ax4)\n",
    "        sns.despine()    \n",
    "        uncertainty_est = uncertainty_est_median_entropy[np.where(correct == False)] # Entropy of MISSED\n",
    "        sns.distplot(uncertainty_est, bins=bins, hist=True, kde=True, rug=False, fit=None, \n",
    "                     hist_kws=None, kde_kws=None, rug_kws=None, fit_kws=None, \n",
    "                     color='r', vertical=False, norm_hist=False, \n",
    "                     axlabel='ENTROPY w.r.t. mean', label='missed', ax=ax4)\n",
    "        sns.despine()\n",
    "        \n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "def hist_uncertainty_binary(result, num_bins, density, cumulative):\n",
    "    labels_1hot_tr = result['train_labels_1hot'] # Mx5\n",
    "    labels_1hot_val = result['val_labels_1hot']\n",
    "    labels_1hot_te = result['test_labels_1hot']\n",
    "    predictions_1hot_tr_ttaug = result['train_pred_1hot'] # MxTx5\n",
    "    predictions_1hot_val_ttaug = result['val_pred_1hot']\n",
    "    predictions_1hot_te_ttaug = result['test_pred_1hot']\n",
    "    \n",
    "    # TRAINING\n",
    "#     print('Training results:')\n",
    "#     hist_uncertainty_binary_helper(labels_1hot_tr, predictions_1hot_tr_ttaug, num_bins, density, cumulative)\n",
    "#     # VALIDATION\n",
    "#     print('Validation results:')\n",
    "#     hist_uncertainty_binary_helper(labels_1hot_val, predictions_1hot_val_ttaug, num_bins, density, cumulative)\n",
    "    # TEST\n",
    "    print('Test results:')\n",
    "    hist_uncertainty_binary_helper(labels_1hot_te, predictions_1hot_te_ttaug, num_bins, density, cumulative)\n",
    "\n",
    "\n",
    "# T_values = [4, 8, 16] #, 32, 64] #, 128]\n",
    "for T in T_values:\n",
    "    print('T = %g' % T)\n",
    "    result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '.pkl'\n",
    "    \n",
    "    with open(result_file_name, 'rb') as filehandler:\n",
    "        result_ttaug = pickle.load(filehandler)\n",
    "        hist_uncertainty_binary(result_ttaug, num_bins=100, density=True, cumulative=False)\n",
    "\n",
    "del result_ttaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# UNCERTAINTY-INFORMED DECISION REFERRAL\n",
    "def decision_referral_helper(labels_1hot, predictions_1hot_ttaug, baseline):\n",
    "    onset_levels = [1, 2]\n",
    "    dec_ref_rates = np.divide(range(0, 50, 1), 100)\n",
    "        \n",
    "    i=0\n",
    "    f = plt.figure(figsize=(15,7.5))\n",
    "    for onset_level in onset_levels:\n",
    "        labels_bin = np.greater_equal(np.argmax(labels_1hot, axis=1), onset_level)\n",
    "        predictions_all_bin = np.sum(predictions_1hot_ttaug[:, :, onset_level:], axis=2) # MxT\n",
    "        \n",
    "        # IQR and median\n",
    "        uncertainty_est_median = stats.iqr(predictions_all_bin, axis=1) # IQR from MxT matrix\n",
    "        predictions_bin_score_median = np.median(predictions_all_bin, axis=1, keepdims=True) # Mx1\n",
    "        \n",
    "        # STD and mean\n",
    "        uncertainty_est_mean = np.std(predictions_all_bin, axis=1) # STD from MxT matrix\n",
    "        predictions_bin_score_mean = np.mean(predictions_all_bin, axis=1, keepdims=True) # Mx1\n",
    "                \n",
    "        # Entropy for median predictions\n",
    "        softmax_median = np.concatenate((np.subtract(1.,predictions_bin_score_median), predictions_bin_score_median), axis=1) # Mx2\n",
    "        uncertainty_est_median_entropy = entropy(softmax_median) # ENT from Mx2 matrix\n",
    "        \n",
    "        # Entropy for mean predictions\n",
    "        softmax_mean = np.concatenate((np.subtract(1.,predictions_bin_score_mean), predictions_bin_score_mean), axis=1) # Mx2\n",
    "        uncertainty_est_mean_entropy = entropy(softmax_mean) # ENT from Mx2 matrix\n",
    "        \n",
    "        ax = f.add_subplot(1, 2, onset_level)\n",
    "        roc_auc_col = []\n",
    "        ref_fraction_col = []\n",
    "        scheme_col = []\n",
    "        \n",
    "        # Decision referral w.r.t. median/IQR\n",
    "        AUCs = [];\n",
    "        randAUCs = [];\n",
    "        for d in range(len(dec_ref_rates)):\n",
    "            # num of items to drop off the end of list (most uncertains towards the end)\n",
    "            drop_count = int(np.round(dec_ref_rates[d]*len(uncertainty_est_median)))\n",
    "            \n",
    "            if drop_count == 0:\n",
    "                drop_count = 1\n",
    "            \n",
    "            rand_idx = np.random.permutation(len(uncertainty_est_median))\n",
    "            rand_idx = rand_idx[:-drop_count]\n",
    "            \n",
    "            idx = np.argsort(uncertainty_est_median) # ascending order, so most uncertain at the end\n",
    "            idx = idx[:-drop_count]         \n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(labels_bin[idx], predictions_bin_score_median[idx])\n",
    "            AUCs.append(auc(fpr, tpr))\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(labels_bin[rand_idx], predictions_bin_score_median[rand_idx])\n",
    "            randAUCs.append(auc(fpr, tpr))           \n",
    "        \n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(randAUCs, newshape=(len(randAUCs),))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        ref_fraction_col = np.concatenate([ref_fraction_col, \n",
    "                                           np.reshape(dec_ref_rates, newshape=(len(dec_ref_rates),))], \n",
    "                                          axis=0\n",
    "                                         )\n",
    "        scheme_col = np.concatenate([scheme_col, \n",
    "                                     np.reshape((('Random, IQR',)*len(randAUCs)), newshape=(len(randAUCs),))], \n",
    "                                    axis=0\n",
    "                                   )\n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(AUCs, newshape=(len(AUCs),))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        ref_fraction_col = np.concatenate([ref_fraction_col, \n",
    "                                           np.reshape(dec_ref_rates, newshape=(len(dec_ref_rates),))], \n",
    "                                          axis=0\n",
    "                                         )\n",
    "        scheme_col = np.concatenate([scheme_col, \n",
    "                                     np.reshape((('Informed, IQR',)*len(AUCs)), newshape=(len(AUCs),))], \n",
    "                                    axis=0\n",
    "                                   )\n",
    "        \n",
    "        # Decision referral w.r.t. mean/STD\n",
    "        AUCs = [];\n",
    "        randAUCs = [];\n",
    "        for d in range(len(dec_ref_rates)):\n",
    "            # num of items to drop off the end of list (most uncertains towards the end)\n",
    "            drop_count = int(np.round(dec_ref_rates[d]*len(uncertainty_est_mean)))\n",
    "            \n",
    "            if drop_count == 0:\n",
    "                drop_count = 1\n",
    "            \n",
    "            rand_idx = np.random.permutation(len(uncertainty_est_mean))\n",
    "            rand_idx = rand_idx[:-drop_count]\n",
    "            \n",
    "            idx = np.argsort(uncertainty_est_mean) # ascending order, so most uncertain at the end\n",
    "            idx = idx[:-drop_count]         \n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(labels_bin[idx], predictions_bin_score_mean[idx])\n",
    "            AUCs.append(auc(fpr, tpr))\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(labels_bin[rand_idx], predictions_bin_score_mean[rand_idx])\n",
    "            randAUCs.append(auc(fpr, tpr))           \n",
    "        \n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(randAUCs, newshape=(len(randAUCs),))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        ref_fraction_col = np.concatenate([ref_fraction_col, \n",
    "                                           np.reshape(dec_ref_rates, newshape=(len(dec_ref_rates),))], \n",
    "                                          axis=0\n",
    "                                         )\n",
    "        scheme_col = np.concatenate([scheme_col, \n",
    "                                     np.reshape((('Random, STD',)*len(randAUCs)), newshape=(len(randAUCs),))], \n",
    "                                    axis=0\n",
    "                                   )\n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(AUCs, newshape=(len(AUCs),))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        ref_fraction_col = np.concatenate([ref_fraction_col, \n",
    "                                           np.reshape(dec_ref_rates, newshape=(len(dec_ref_rates),))], \n",
    "                                          axis=0\n",
    "                                         )\n",
    "        scheme_col = np.concatenate([scheme_col, \n",
    "                                     np.reshape((('Informed, STD',)*len(AUCs)), newshape=(len(AUCs),))], \n",
    "                                    axis=0\n",
    "                                   )\n",
    "        \n",
    "        # Decision referral w.r.t. entropy{median}\n",
    "        AUCs = [];\n",
    "        randAUCs = [];\n",
    "        for d in range(len(dec_ref_rates)):\n",
    "            # num of items to drop off the end of list (most uncertains towards the end)\n",
    "            drop_count = int(np.round(dec_ref_rates[d]*len(uncertainty_est_median_entropy)))\n",
    "            \n",
    "            if drop_count == 0:\n",
    "                drop_count = 1\n",
    "            \n",
    "            rand_idx = np.random.permutation(len(uncertainty_est_median_entropy))\n",
    "            rand_idx = rand_idx[:-drop_count]\n",
    "            \n",
    "            idx = np.argsort(uncertainty_est_median_entropy) # ascending order, so most uncertain at the end\n",
    "            idx = idx[:-drop_count]         \n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(labels_bin[idx], predictions_bin_score_median[idx])\n",
    "            AUCs.append(auc(fpr, tpr))\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(labels_bin[rand_idx], predictions_bin_score_median[rand_idx])\n",
    "            randAUCs.append(auc(fpr, tpr)) \n",
    "        \n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(randAUCs, newshape=(len(randAUCs),))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        ref_fraction_col = np.concatenate([ref_fraction_col, \n",
    "                                           np.reshape(dec_ref_rates, newshape=(len(dec_ref_rates),))], \n",
    "                                          axis=0\n",
    "                                         )\n",
    "        scheme_col = np.concatenate([scheme_col, \n",
    "                                     np.reshape((('Random, ENT (median)',)*len(randAUCs)), newshape=(len(randAUCs),))], \n",
    "                                    axis=0\n",
    "                                   )                \n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(AUCs, newshape=(len(AUCs),))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        ref_fraction_col = np.concatenate([ref_fraction_col, \n",
    "                                           np.reshape(dec_ref_rates, newshape=(len(dec_ref_rates),))], \n",
    "                                          axis=0\n",
    "                                         )\n",
    "        scheme_col = np.concatenate([scheme_col, \n",
    "                                     np.reshape((('Informed, ENT (median)',)*len(AUCs)), newshape=(len(AUCs),))], \n",
    "                                    axis=0\n",
    "                                   )\n",
    "        \n",
    "        # Decision referral w.r.t. entropy{mean}\n",
    "        AUCs = [];\n",
    "        randAUCs = [];\n",
    "        for d in range(len(dec_ref_rates)):\n",
    "            # num of items to drop off the end of list (most uncertains towards the end)\n",
    "            drop_count = int(np.round(dec_ref_rates[d]*len(uncertainty_est_mean_entropy)))\n",
    "            \n",
    "            if drop_count == 0:\n",
    "                drop_count = 1\n",
    "            \n",
    "            rand_idx = np.random.permutation(len(uncertainty_est_mean_entropy))\n",
    "            rand_idx = rand_idx[:-drop_count]\n",
    "            \n",
    "            idx = np.argsort(uncertainty_est_mean_entropy) # ascending order, so most uncertain at the end\n",
    "            idx = idx[:-drop_count]         \n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(labels_bin[idx], predictions_bin_score_mean[idx])\n",
    "            AUCs.append(auc(fpr, tpr))\n",
    "            \n",
    "            fpr, tpr, _ = roc_curve(labels_bin[rand_idx], predictions_bin_score_mean[rand_idx])\n",
    "            randAUCs.append(auc(fpr, tpr))\n",
    "        \n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(randAUCs, newshape=(len(randAUCs),))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        ref_fraction_col = np.concatenate([ref_fraction_col, \n",
    "                                           np.reshape(dec_ref_rates, newshape=(len(dec_ref_rates),))], \n",
    "                                          axis=0\n",
    "                                         )\n",
    "        scheme_col = np.concatenate([scheme_col, \n",
    "                                     np.reshape((('Random, ENT (mean)',)*len(randAUCs)), newshape=(len(randAUCs),))], \n",
    "                                    axis=0\n",
    "                                   )                \n",
    "        roc_auc_col = np.concatenate([roc_auc_col, \n",
    "                                      np.reshape(AUCs, newshape=(len(AUCs),))], \n",
    "                                     axis=0\n",
    "                                    )\n",
    "        ref_fraction_col = np.concatenate([ref_fraction_col, \n",
    "                                           np.reshape(dec_ref_rates, newshape=(len(dec_ref_rates),))], \n",
    "                                          axis=0\n",
    "                                         )\n",
    "        scheme_col = np.concatenate([scheme_col, \n",
    "                                     np.reshape((('Informed, ENT (mean)',)*len(AUCs)), newshape=(len(AUCs),))], \n",
    "                                    axis=0\n",
    "                                   )\n",
    "        \n",
    "        df_dec_ref = pd.DataFrame()\n",
    "        df_dec_ref['ROC-AUC'] = roc_auc_col\n",
    "        df_dec_ref['referral rate'] = ref_fraction_col\n",
    "        df_dec_ref['Scheme'] = scheme_col\n",
    "    \n",
    "        ax = sns.lineplot(x='referral rate', y='ROC-AUC', hue='Scheme', ci=None, \n",
    "                          data=df_dec_ref, ax=ax)\n",
    "        ax.set_title('Onset ' + str(onset_level))\n",
    "        sns.despine()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def decision_referral_plot(result, baseline):\n",
    "    labels_1hot_tr = result['train_labels_1hot'] # Mx5\n",
    "    labels_1hot_val = result['val_labels_1hot']\n",
    "    labels_1hot_te = result['test_labels_1hot']\n",
    "    predictions_1hot_tr_ttaug = result['train_pred_1hot'] # MxTx5\n",
    "    predictions_1hot_val_ttaug = result['val_pred_1hot']\n",
    "    predictions_1hot_te_ttaug = result['test_pred_1hot']\n",
    "    \n",
    "    # TRAINING\n",
    "#     print('Training results:')\n",
    "#     decision_referral_helper(labels_1hot_tr, predictions_1hot_tr_ttaug)\n",
    "    # VALIDATION\n",
    "#     print('Validation results:')\n",
    "#     decision_referral_helper(labels_1hot_val, predictions_1hot_val_ttaug)\n",
    "    # TEST\n",
    "    print('Test results:')\n",
    "    decision_referral_helper(labels_1hot_te, predictions_1hot_te_ttaug, baseline)\n",
    "\n",
    "\n",
    "for T in T_values:\n",
    "    print('T = %g' % T)\n",
    "    result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '.pkl'\n",
    "    \n",
    "    with open(result_file_name, 'rb') as filehandler:\n",
    "        result_ttaug = pickle.load(filehandler)\n",
    "        decision_referral_plot(result_ttaug, baseline)\n",
    "del result_ttaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### QUANTIFY Heteroscedastic Aleatoric Uncertainty via InfoGain/Mutual Information\n",
    "def aleatoric_uncertainty_quantify_helper(labels_1hot, predictions_1hot_ttaug):\n",
    "    onset_levels = [1, 2]\n",
    "#     colors = ['darkslategrey', 'darkgoldenrod']\n",
    "    colors = ['deepskyblue', 'firebrick']\n",
    "    \n",
    "#     f = plt.figure(figsize=(15,6))\n",
    "    i = 0\n",
    "    f, axes = plt.subplots(1, 3, figsize=(15,6))\n",
    "    for onset_level in onset_levels:\n",
    "        print('Onset level: %d'  % onset_level)\n",
    "        labels_bin = np.greater_equal(np.argmax(labels_1hot, axis=1), onset_level)\n",
    "        predictions_all_bin_y1 = np.sum(predictions_1hot_ttaug[:, :, onset_level:], \n",
    "                                        axis=2, keepdims=True) # MxTxC --> MxTx1\n",
    "#         predictions_bin = np.greater_equal(np.mean(predictions_all_bin_y1, axis=1), 0.5)\n",
    "        \n",
    "        predictions_all_bin_y0 = np.subtract(1., predictions_all_bin_y1) # MxTx1\n",
    "        predictions_all_bin_probs = np.concatenate((predictions_all_bin_y0, predictions_all_bin_y1), axis=-1) # MxTx2\n",
    "        \n",
    "        # Now, compute InfoGain/MI\n",
    "        p_ttaug_y1 = np.mean(predictions_all_bin_probs[:,:,1], axis=1, keepdims=True) # MxTx1\n",
    "        p_ttaug_y0 = np.subtract(1., p_ttaug_y1) # MxTx1\n",
    "        p_ttaug = np.concatenate((p_ttaug_y0, p_ttaug_y1), axis=-1) # MxTx2\n",
    "        \n",
    "        predictive_entropy = entropy(p_ttaug, axis=-1, keepdims=False) # M\n",
    "        expected_entropy = np.mean(entropy(predictions_all_bin_probs, axis=-1, keepdims=False), axis=-1, keepdims=False) \n",
    "        MI = np.subtract(predictive_entropy, expected_entropy)\n",
    "        \n",
    "        print('NaNs in Pred. Ent. : ' + str(np.any(np.isnan(predictive_entropy))))\n",
    "        print('NaNs in Exp. Ent. : ' + str(np.any(np.isnan(expected_entropy))))           \n",
    "        print('NaNs in MI : ' + str(np.any(np.isnan(MI))))           \n",
    "        \n",
    "        print('Min MI: %g\\t1st Quar.: %g\\tMean: %g\\tLast Quar: %g\\tMax MI: %g' % \n",
    "              (np.amin(MI), np.percentile(MI, 25), np.mean(MI), np.percentile(MI, 75), np.amax(MI)))\n",
    "        \n",
    "        sort_idx = np.argsort(predictive_entropy)\n",
    "        \n",
    "#         ax = f.add_subplot(1, 3, onset_level)\n",
    "        point_size = 1.5\n",
    "        axes[onset_level-1].scatter(range(len(predictive_entropy)), predictive_entropy[sort_idx], s=point_size, \n",
    "                                  color='m', alpha=0.5, label='predictive entropy')\n",
    "        axes[onset_level-1].scatter(range(len(expected_entropy)), expected_entropy[sort_idx], s=point_size, \n",
    "                                  color='g', alpha=0.5, label='expected entropy')\n",
    "        axes[onset_level-1].scatter(range(len(MI)), MI[sort_idx], s=point_size, \n",
    "                                  color='b', alpha=0.5, label='InfoGain/MutualInfo')\n",
    "        axes[onset_level-1].set_xlabel('Examples sorted by predictive entropy')\n",
    "        axes[onset_level-1].set_label('entropy')\n",
    "        axes[onset_level-1].legend(shadow=True, fancybox=True, markerscale=3, handletextpad=0.1)\n",
    "        axes[onset_level-1].grid(True)\n",
    "        \n",
    "        # Relative gain        \n",
    "        temp = np.sort(np.divide(predictive_entropy, expected_entropy))\n",
    "        axes[-1].scatter(range(len(temp)), temp, color=colors[i], alpha=0.5, s=point_size, \n",
    "                         label='pred. ent. / exp. ent., onset ' + str(onset_level))\n",
    "        i = i + 1\n",
    "    \n",
    "    axes[-1].set_xlabel('Examples sorted by the relative gain')\n",
    "    axes[-1].set_ylabel('relative gain')\n",
    "    axes[-1].legend(shadow=True, fancybox=True, markerscale=3, handletextpad=0.1)\n",
    "    axes[-1].grid(True)\n",
    "    plt.show()   \n",
    "    \n",
    "\n",
    "def aleatoric_uncertainty_quantify_plot(result):\n",
    "    labels_1hot_tr = result['train_labels_1hot'] # Mx5\n",
    "    labels_1hot_val = result['val_labels_1hot']\n",
    "    labels_1hot_te = result['test_labels_1hot']\n",
    "    predictions_1hot_tr_ttaug = result['train_pred_1hot'] # MxTx5\n",
    "    predictions_1hot_val_ttaug = result['val_pred_1hot']\n",
    "    predictions_1hot_te_ttaug = result['test_pred_1hot']\n",
    "    \n",
    "    # TRAINING\n",
    "#     print('Training results:')\n",
    "#     aleatoric_uncertainty_quantify_helper(labels_1hot_tr, predictions_1hot_tr_ttaug)\n",
    "    # VALIDATION\n",
    "#     print('Validation results:')\n",
    "#     aleatoric_uncertainty_quantify_helper(labels_1hot_val, predictions_1hot_val_ttaug)\n",
    "    # TEST\n",
    "    print('Test results:')\n",
    "    aleatoric_uncertainty_quantify_helper(labels_1hot_te, predictions_1hot_te_ttaug)\n",
    "\n",
    "for T in T_values:\n",
    "    print('T = %g' % T)\n",
    "    result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '.pkl'\n",
    "    \n",
    "    with open(result_file_name, 'rb') as filehandler:\n",
    "        result_ttaug = pickle.load(filehandler)\n",
    "        aleatoric_uncertainty_quantify_plot(result_ttaug)\n",
    "\n",
    "del result_ttaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relative_gain_plot_helper(labels_1hot, predictions_1hot_ttaug, axes, i, T):\n",
    "    onset_levels = [1, 2]\n",
    "#     colors = ['burlywood','maroon', 'darkslateblue', 'lightskyblue', 'fuchsia', 'rosybrown'] # ['darkslategrey', 'darkgoldenrod']\n",
    "    colors = ['r','g', 'b', 'maroon', 'fuchsia', 'lightskyblue']\n",
    "    point_size = 1.0\n",
    "    \n",
    "    for onset_level in onset_levels:\n",
    "#         print('Onset level: %d'  % onset_level)\n",
    "        labels_bin = np.greater_equal(np.argmax(labels_1hot, axis=1), onset_level)\n",
    "        predictions_all_bin_y1 = np.sum(predictions_1hot_ttaug[:, :, onset_level:], \n",
    "                                        axis=2, keepdims=True) # MxTxC --> MxTx1\n",
    "#         predictions_bin = np.greater_equal(np.mean(predictions_all_bin_y1, axis=1), 0.5)\n",
    "        \n",
    "        predictions_all_bin_y0 = np.subtract(1., predictions_all_bin_y1) # MxTx1\n",
    "        predictions_all_bin_probs = np.concatenate((predictions_all_bin_y0, predictions_all_bin_y1), axis=-1) # MxTx2\n",
    "        \n",
    "        # Now, compute InfoGain/MI\n",
    "        p_ttaug_y1 = np.mean(predictions_all_bin_probs[:,:,1], axis=1, keepdims=True) # MxTx1\n",
    "        p_ttaug_y0 = np.subtract(1., p_ttaug_y1) # MxTx1\n",
    "        p_ttaug = np.concatenate((p_ttaug_y0, p_ttaug_y1), axis=-1) # MxTx2\n",
    "        \n",
    "        predictive_entropy = entropy(p_ttaug, axis=-1, keepdims=False) # M\n",
    "        expected_entropy = np.mean(entropy(predictions_all_bin_probs, axis=-1, keepdims=False), axis=-1, keepdims=False) \n",
    "        \n",
    "        # now, sort w.r.t. relative gain and plot\n",
    "        relative_gain = np.divide(predictive_entropy, expected_entropy)\n",
    "        temp = np.sort(relative_gain)\n",
    "        # trim the first 50 and last 50 to emphasize the midrange of gain in the figure\n",
    "#         temp = temp[100:]\n",
    "        temp = temp[:-500]\n",
    "        axes[onset_level-1].scatter(range(len(temp)), temp, color=colors[i], alpha=0.5, s=point_size, \n",
    "                                    label='pred. ent. / exp. ent., T=' + str(T))\n",
    "#         axes[onset_level-1].plot(range(len(temp)), temp, color=colors[i], linestyle='--', \n",
    "#                                  label='pred. ent. / exp. ent., T=' + str(T))\n",
    "        axes[onset_level-1].set_xlabel('Examples sorted by the relative gain')\n",
    "        axes[onset_level-1].set_ylabel('relative gain')\n",
    "        axes[onset_level-1].set_title('Onset ' + str(onset_level))\n",
    "        axes[onset_level-1].legend(shadow=True, fancybox=True, markerscale=5, handletextpad=0.1)\n",
    "        axes[onset_level-1].grid(True)\n",
    "\n",
    "\n",
    "def relative_gain_plot(result, axes, i, T):\n",
    "    labels_1hot_tr = result['train_labels_1hot'] # Mx5\n",
    "    labels_1hot_val = result['val_labels_1hot']\n",
    "    labels_1hot_te = result['test_labels_1hot']\n",
    "    predictions_1hot_tr_ttaug = result['train_pred_1hot'] # MxTx5\n",
    "    predictions_1hot_val_ttaug = result['val_pred_1hot']\n",
    "    predictions_1hot_te_ttaug = result['test_pred_1hot']\n",
    "    \n",
    "    # TRAINING\n",
    "#     print('Training results:')\n",
    "#     relative_gain_plot_helper(labels_1hot_tr, predictions_1hot_tr_ttaug)\n",
    "    # VALIDATION\n",
    "#     print('Validation results:')\n",
    "#     relative_gain_plot_helper(labels_1hot_val, predictions_1hot_val_ttaug)\n",
    "    # TEST\n",
    "    print('Test results:')\n",
    "    relative_gain_plot_helper(labels_1hot_te, predictions_1hot_te_ttaug, axes, i, T)\n",
    "\n",
    "\n",
    "i=0\n",
    "f, axes = plt.subplots(1, 2, figsize=(16,8))\n",
    "for T in T_values:\n",
    "    print('T = %g' % T)\n",
    "    result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '.pkl'\n",
    "\n",
    "    with open(result_file_name, 'rb') as filehandler:\n",
    "        result_ttaug = pickle.load(filehandler)\n",
    "        relative_gain_plot(result_ttaug, axes, i, T)\n",
    "    i += 1\n",
    "plt.show()\n",
    "\n",
    "del result_ttaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ALIGNED t-SNE Maps\n",
    "from FItSNE.fast_tsne import fast_tsne \n",
    "# import scipy.spatial as sp\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import colors\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "def find_kNN_idx_per_column(col, k):\n",
    "    sorted_idx = np.argsort(col) # asceding order\n",
    "    return sorted_idx[:k] # return the closest (top) k\n",
    "\n",
    "def find_kNN_inits_per_column(kNN_idx, Z, dims=2):\n",
    "    return Z[kNN_idx,:dims]\n",
    "\n",
    "def plot_given_map(Z, y, ax, title, point_size=2, markerscale=5, plotting_order='original'):\n",
    "    col = np.array(['burlywood','maroon', 'lightskyblue', 'darkslateblue', 'fuchsia'])    \n",
    "    legend_labels = np.array(['0: No DR', '1: Mild DR', '2: Moderate DR', '3: Severe DR', '4: Proliferative DR'])\n",
    "    \n",
    "    if plotting_order == 'original':\n",
    "        plotting_order = range(len(np.unique(y)))\n",
    "    else:\n",
    "        plotting_order = [0,2,3,4,1] #[4,3,0,2,1]\n",
    "    \n",
    "    for i in plotting_order: # for i in range(len(np.unique(y))):\n",
    "        mask = (y == i)\n",
    "        ax.scatter(Z[mask,0], Z[mask,1], c=col[y[mask]], label=legend_labels[i], s=point_size)\n",
    "    ax.set_title(title)\n",
    "    leg = ax.legend(bbox_to_anchor=(0., 1.125, 1., 0.), loc='upper center', ncol=3, \n",
    "                    mode=\"expand\", shadow=True, fancybox=True, markerscale=markerscale, handletextpad=0.1)\n",
    "    leg.get_frame().set_alpha(0.75)\n",
    "    \n",
    "def plot_aligned_tsne(Xa, ya, \n",
    "                      Xb, yb, \n",
    "                      ax1, ax2, \n",
    "                      perplexity=30, max_iter=1000, \n",
    "                      variance_to_keep=0.99, k=10,\n",
    "                      multicore_kNN=False, num_cores=10,\n",
    "                      plotting_order='original'):\n",
    "    \n",
    "    ###### First, standardize the data\n",
    "    scaler = preprocessing.StandardScaler() # Zero mean, unit variance\n",
    "    Xa = scaler.fit_transform(Xa)\n",
    "    Xb = scaler.transform(Xb)\n",
    "    \n",
    "    ###### Compute the pairwise distances and determined kNNs\n",
    "    print('Computing the pairwise distances')\n",
    "#     K = sp.distance.cdist(Xa, Xb, metric='euclidean')  # matrix of pairwise distances\n",
    "    K = pairwise_distances(X=Xa, Y=Xb, metric='euclidean') \n",
    "    Ma, Mb = K.shape\n",
    "    print('Finding kNNs...')\n",
    "    kNN_idx_list = []\n",
    "    if not multicore_kNN:\n",
    "        for j in range(Mb): # loop over the items to be aligned with the reference map from Xa.\n",
    "            idx = np.argsort(K[:,j]) # ascending order, so most distant at the end. kNNs are in the front\n",
    "            kNN_idx_list.append(idx[:k]) # append the kNN indices\n",
    "    else:\n",
    "        kNNs_by_idx = Parallel(n_jobs=num_cores)(delayed(find_kNN_idx_per_column)(K[:,j], k) for j in range(Mb))\n",
    "        for j in range(len(kNNs_by_idx)):\n",
    "            kNN_idx_list.append(kNNs_by_idx[j])\n",
    "           \n",
    "    ###### Do PCA on the reference data and keep D dimensions\n",
    "    print('PCA on reference data ...')\n",
    "    Sigma = np.cov(np.transpose(Xa))\n",
    "    U, s, V = np.linalg.svd(Sigma, full_matrices=False)\n",
    "    sum_s = np.sum(s)\n",
    "    print('Total components : %g' % len(s))\n",
    "    for d in range(len(s)):\n",
    "        var_explained = np.sum(s[:d]) / sum_s\n",
    "        if var_explained >= variance_to_keep:\n",
    "            break\n",
    "    print('%g of variance explained with %d components.' % (var_explained, d))\n",
    "    D = d\n",
    "    XaD = np.dot(Xa, U[:,:D])   # np.dot(U, np.diag(s))[:,:D]\n",
    "    PCAinit = XaD[:,:2] / np.std(XaD[:,0]) * 0.0001\n",
    "    \n",
    "    ####### tSNE on the reference (training) data\n",
    "    print('Computing tSNE map for the reference data')\n",
    "    Za = fast_tsne(XaD, perplexity=perplexity,\n",
    "                   max_iter=max_iter, learning_rate=learning_rate,\n",
    "#                    stop_early_exag_iter=int(max_iter*0.25), early_exag_coeff=12,\n",
    "#                    start_late_exag_iter=int(max_iter*0.90), late_exag_coeff=2, \n",
    "                   initialization=PCAinit)\n",
    "    plot_given_map(Za, ya, ax1, 'tSNE map, reference data, ' + ' Perplexity : ' + str(perplexity),\n",
    "                   plotting_order=plotting_order)\n",
    "\n",
    "    ####################################################################\n",
    "    ###### ALIGNMENT begins\n",
    "    XbD = np.dot(Xb, U[:,:D])    # np.dot(U, np.diag(s))[:,:D]\n",
    "\n",
    "    print('Collecting initialization points based on kNNs')\n",
    "    kNN_init = []\n",
    "    if not multicore_kNN:\n",
    "        for kNN_idx in kNN_idx_list:\n",
    "            kNNs = Za[kNN_idx,:2]\n",
    "            kNN_init.append(np.mean(kNNs, axis=0))\n",
    "            kNN_init = np.reshape(kNN_init, newshape=(len(kNN_idx_list),2))\n",
    "    else:\n",
    "        kNN_init = Parallel(n_jobs=num_cores)(delayed(find_kNN_inits_per_column)(kNN_idx_list[j], Za, 2) for j in range(len(kNN_idx_list)))\n",
    "        kNN_init = np.reshape(kNN_init, newshape=(len(kNN_idx_list), k, 2))\n",
    "        kNN_init = np.mean(kNN_init, axis=1)\n",
    "#     kNN_init = kNN_init[:,:2] / np.std(kNN_init[:,0]) * 0.0001\n",
    "    \n",
    "    print('Computing tSNE map for the auxillary data')\n",
    "    Zb = fast_tsne(XbD, perplexity=perplexity,\n",
    "                   max_iter=max_iter, learning_rate=learning_rate,\n",
    "#                    stop_early_exag_iter=int(max_iter*0.25), early_exag_coeff=12,\n",
    "#                    start_late_exag_iter=int(max_iter*0.90), late_exag_coeff=2, \n",
    "                   initialization=kNN_init)\n",
    "\n",
    "    # Concatenate the mappings and plot\n",
    "    Z = np.concatenate([Za,Zb], axis=0)\n",
    "    y = np.concatenate([ya,yb], axis=0)   \n",
    "    plot_given_map(Z, y, ax2, 'Aligned tSNE, ' + ' Perplexity : ' + str(perplexity),\n",
    "                   plotting_order=plotting_order)\n",
    "\n",
    "# Now, read the SINGLE PRED. results from file and plot\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_SINGpred.pkl'\n",
    "\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    result = pickle.load(filehandler)\n",
    "    \n",
    "    X_tr = result['train_features'] \n",
    "    X_val = result['val_features']\n",
    "    X_te = result['test_features']\n",
    "    \n",
    "    y_tr = np.argmax(result['train_labels_1hot'], axis=1)\n",
    "    y_val = np.argmax(result['val_labels_1hot'], axis=1)\n",
    "    y_te = np.argmax(result['test_labels_1hot'], axis=1)\n",
    "\n",
    "X_valte = np.concatenate([X_val, X_te], axis=0)\n",
    "y_valte = np.concatenate([y_val, y_te], axis=0)\n",
    "# del result, X_val, y_val, X_te, y_te, # make some room\n",
    "    \n",
    "\n",
    "perplexities = [100,500,1000,2000] # [10,20,30,40,50,100]\n",
    "variance_to_keep = 0.99\n",
    "num_neighbors = 10\n",
    "max_iter = 3000\n",
    "learning_rate = 500\n",
    "plotting_order='my_order'\n",
    "for perp in perplexities:\n",
    "    print('Perplexity : %g' % perp)\n",
    "    f = plt.figure(figsize=(15,7.5))\n",
    "    ax1 = f.add_subplot(1, 2, 1)\n",
    "    ax2 = f.add_subplot(1, 2, 2)\n",
    "    \n",
    "    plot_aligned_tsne(X_tr, y_tr, \n",
    "                      X_valte, y_valte, \n",
    "                      ax1, ax2, \n",
    "                      perplexity=perp, max_iter=max_iter, \n",
    "                      variance_to_keep=variance_to_keep, k=num_neighbors, \n",
    "                      multicore_kNN=True, num_cores=multiprocessing.cpu_count(),\n",
    "                      plotting_order=plotting_order)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "del result, X_val, y_val, X_te, y_te, # make some room\n",
    "del X_tr, y_tr, X_valte, y_valte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ALIGNED t-SNE Maps with a bit of tSNE LOGIC\n",
    "from FItSNE.fast_tsne import fast_tsne \n",
    "# import scipy.spatial as sp\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import colors\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "def plot_aligned_tsne_with_labels_and_predictions(Xa, ya, y_pred_a, y_pred_a_ttaug,\n",
    "                                                  Xb, yb, y_pred_b, y_pred_b_ttaug,\n",
    "                                                  ax1, ax2, ax3, ax4, ax5,\n",
    "                                                  perplexity=30, max_iter=1000, \n",
    "                                                  variance_to_keep=0.99, k=10,\n",
    "                                                  multicore_kNN=False, num_cores=10,\n",
    "                                                  plotting_order='original'):\n",
    "    \n",
    "    ###### First, standardize the data\n",
    "    scaler = preprocessing.StandardScaler() # Zero mean, unit variance\n",
    "    Xa = scaler.fit_transform(Xa)\n",
    "    Xb = scaler.transform(Xb)\n",
    "    \n",
    "    ###### Compute the pairwise distances and determined kNNs\n",
    "    print('Computing the pairwise distances')\n",
    "#     K = sp.distance.cdist(Xa, Xb, metric='euclidean')  # matrix of pairwise distances\n",
    "    K = pairwise_distances(X=Xa, Y=Xb, metric='euclidean') \n",
    "    Ma, Mb = K.shape\n",
    "    print('Finding kNNs...')\n",
    "    kNN_idx_list = []\n",
    "    if not multicore_kNN:\n",
    "        for j in range(Mb): # loop over the items to be aligned with the reference map from Xa.\n",
    "            idx = np.argsort(K[:,j]) # ascending order, so most distant at the end. kNNs are in the front\n",
    "            kNN_idx_list.append(idx[:k]) # append the kNN indices\n",
    "    else:\n",
    "        kNNs_by_idx = Parallel(n_jobs=num_cores)(delayed(find_kNN_idx_per_column)(K[:,j], k) for j in range(Mb))\n",
    "        for j in range(len(kNNs_by_idx)):\n",
    "            kNN_idx_list.append(kNNs_by_idx[j])\n",
    "           \n",
    "    ###### Do PCA on the reference data and keep D dimensions\n",
    "    print('PCA on reference data ...')\n",
    "    Sigma = np.cov(np.transpose(Xa))\n",
    "    U, s, V = np.linalg.svd(Sigma, full_matrices=False)\n",
    "    sum_s = np.sum(s)\n",
    "    print('Total components : %g' % len(s))\n",
    "    for d in range(len(s)):\n",
    "        var_explained = np.sum(s[:d]) / sum_s\n",
    "        if var_explained >= variance_to_keep:\n",
    "            break\n",
    "    print('%g of variance explained with %d components.' % (var_explained, d))\n",
    "    D = d\n",
    "    XaD = np.dot(Xa, U[:,:D])   # np.dot(U, np.diag(s))[:,:D]\n",
    "    PCAinit = XaD[:,:2] / np.std(XaD[:,0]) * 0.0001\n",
    "    \n",
    "    ####### tSNE on the reference (training) data\n",
    "    print('Computing tSNE map for the reference data')\n",
    "    Za = fast_tsne(XaD, perplexity=perplexity,\n",
    "                   max_iter=max_iter, learning_rate=learning_rate,\n",
    "#                    stop_early_exag_iter=int(max_iter*0.25), early_exag_coeff=12,\n",
    "#                    start_late_exag_iter=int(max_iter*0.90), late_exag_coeff=2, \n",
    "                   initialization=PCAinit)\n",
    "    ## DO NOT Plot the mappings for reference data, NOW!\n",
    "#     plot_given_map(Za, ya, ax1, 'tSNE map, reference data, ' + ' Perplexity : ' + str(perplexity), \n",
    "# plotting_order=plotting_order)\n",
    "    \n",
    "    \n",
    "    ####################################################################\n",
    "    ###### ALIGNMENT begins\n",
    "    XbD = np.dot(Xb, U[:,:D])    # np.dot(U, np.diag(s))[:,:D]\n",
    "\n",
    "    print('Collecting initialization points based on kNNs')\n",
    "    kNN_init = []\n",
    "    if not multicore_kNN:\n",
    "        for kNN_idx in kNN_idx_list:\n",
    "            kNNs = Za[kNN_idx,:2]\n",
    "            kNN_init.append(np.mean(kNNs, axis=0))\n",
    "            kNN_init = np.reshape(kNN_init, newshape=(len(kNN_idx_list),2))\n",
    "    else:\n",
    "        kNN_init = Parallel(n_jobs=num_cores)(delayed(find_kNN_inits_per_column)(kNN_idx_list[j], Za, 2) for j in range(len(kNN_idx_list)))\n",
    "        kNN_init = np.reshape(kNN_init, newshape=(len(kNN_idx_list), k, 2))\n",
    "        kNN_init = np.mean(kNN_init, axis=1)\n",
    "#     kNN_init = kNN_init[:,:2] / np.std(kNN_init[:,0]) * 0.0001\n",
    "    \n",
    "    print('Computing tSNE map for the auxillary data')\n",
    "    Zb = fast_tsne(XbD, perplexity=perplexity,\n",
    "                   max_iter=max_iter, learning_rate=learning_rate,\n",
    "#                    stop_early_exag_iter=int(max_iter*0.25), early_exag_coeff=12,\n",
    "#                    start_late_exag_iter=int(max_iter*0.90), late_exag_coeff=2, \n",
    "                   initialization=kNN_init)\n",
    "\n",
    "    # Concatenate the mappings and plot\n",
    "    Z = np.concatenate([Za,Zb], axis=0)\n",
    "    y = np.concatenate([ya,yb], axis=0)\n",
    "    y_pred = np.concatenate([y_pred_a, y_pred_b], axis=0)\n",
    "    y_pred_ttaug = np.concatenate([y_pred_a_ttaug, y_pred_b_ttaug], axis=0)\n",
    "    \n",
    "    point_size = 2\n",
    "    markerscale = 4\n",
    "    \n",
    "    plot_given_map(Z, y, \n",
    "                   ax1, 'LABELS, ' + 'Perplexity : ' + str(perplexity), \n",
    "                   point_size=point_size, \n",
    "                   markerscale=markerscale, plotting_order=plotting_order)\n",
    "    \n",
    "    # Now, plot the tSNE maps with predictions, instead of labels, for comparison\n",
    "    plot_given_map(Z, y_pred, \n",
    "                   ax2, 'SINGLE pred., ' + 'Perplexity : ' + str(perplexity),\n",
    "                   point_size=point_size,\n",
    "                   markerscale=markerscale, plotting_order=plotting_order)\n",
    "    plot_given_map(Z, y_pred_ttaug, \n",
    "                   ax3, 'TTAUG pred., ' + 'Perplexity : ' + str(perplexity),\n",
    "                   point_size=point_size,\n",
    "                   markerscale=markerscale, plotting_order=plotting_order)\n",
    "        \n",
    "    masked_preds = np.not_equal(y, y_pred) # find the wrong predictions\n",
    "    masked_pred_idx = np.where(masked_preds == True)[0] # take the first in tuple!!! OMG!\n",
    "    plot_given_map(Z[masked_pred_idx,:], y[masked_pred_idx], \n",
    "                   ax4, 'WRONG SINGLE pred., ' + 'Perplexity : ' + str(perplexity) + \n",
    "                   ' , # : ' + str(np.sum(masked_preds)),\n",
    "                   point_size=point_size,\n",
    "                   markerscale=markerscale, plotting_order=plotting_order)\n",
    "    \n",
    "    masked_preds = np.not_equal(y, y_pred_ttaug) # find the wrong predictions\n",
    "    masked_pred_idx = np.where(masked_preds == True)[0]\n",
    "    plot_given_map(Z[masked_pred_idx,:], y[masked_pred_idx], \n",
    "                   ax5, 'WRONG TTAUG pred., ' + 'Perplexity : ' + str(perplexity) + \n",
    "                   ' , # : ' + str(np.sum(masked_preds)), \n",
    "                   point_size=point_size,\n",
    "                   markerscale=markerscale, plotting_order=plotting_order)\n",
    "\n",
    "# Now, read the SINGLE PRED. results from file and plot\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_SINGpred.pkl'\n",
    "\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    result = pickle.load(filehandler)\n",
    "    \n",
    "    X_tr = result['train_features'] \n",
    "    X_val = result['val_features']\n",
    "    X_te = result['test_features']\n",
    "    \n",
    "    y_tr = np.argmax(result['train_labels_1hot'], axis=1)\n",
    "    y_val = np.argmax(result['val_labels_1hot'], axis=1)\n",
    "    y_te = np.argmax(result['test_labels_1hot'], axis=1)\n",
    "    \n",
    "    pred_tr = np.argmax(result['train_pred_1hot'], axis=1)\n",
    "    pred_val = np.argmax(result['val_pred_1hot'], axis=1)\n",
    "    pred_te = np.argmax(result['test_pred_1hot'], axis=1)\n",
    "\n",
    "X_valte = np.concatenate([X_val, X_te], axis=0)\n",
    "y_valte = np.concatenate([y_val, y_te], axis=0)\n",
    "pred_valte = np.concatenate([pred_val, pred_te], axis=0)\n",
    "del result, X_val, y_val, pred_val, X_te, y_te, pred_te # make some room\n",
    "    \n",
    "\n",
    "# Get the predictions from TTAUG results\n",
    "T = 128\n",
    "mode = 'mean'\n",
    "# for T in T_values:\n",
    "print('T = %g' % T)\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '.pkl'\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    result_ttaug = pickle.load(filehandler)\n",
    "    \n",
    "    if mode == 'mean':\n",
    "        pred_tr_ttaug = np.argmax(np.mean(result_ttaug['train_pred_1hot'], axis=1), axis=-1)\n",
    "        pred_val_ttaug = np.argmax(np.mean(result_ttaug['val_pred_1hot'], axis=1), axis=-1)\n",
    "        pred_te_ttaug = np.argmax(np.mean(result_ttaug['test_pred_1hot'], axis=1), axis=-1)\n",
    "    else: # median\n",
    "        pred_tr_ttaug = np.argmax(np.median(result_ttaug['train_pred_1hot'], axis=1), axis=-1)\n",
    "        pred_val_ttaug = np.argmax(np.median(result_ttaug['val_pred_1hot'], axis=1), axis=-1)\n",
    "        pred_te_ttaug = np.argmax(np.median(result_ttaug['test_pred_1hot'], axis=1), axis=-1)\n",
    "\n",
    "pred_valte_ttaug = np.concatenate([pred_val_ttaug, pred_te_ttaug], axis=0)\n",
    "del pred_val_ttaug, pred_te_ttaug\n",
    "\n",
    "\n",
    "for perp in perplexities:\n",
    "    print('Perplexity : %g' % perp)\n",
    "    f = plt.figure(figsize=(22,15))\n",
    "    ax1 = f.add_subplot(2, 3, 1) \n",
    "    ax2 = f.add_subplot(2, 3, 2) \n",
    "    ax3 = f.add_subplot(2, 3, 3) \n",
    "    ax4 = f.add_subplot(2, 3, 5) # skip 4\n",
    "    ax5 = f.add_subplot(2, 3, 6) \n",
    "    plot_aligned_tsne_with_labels_and_predictions(X_tr, y_tr, pred_tr, pred_tr_ttaug, \n",
    "                                                  X_valte, y_valte, pred_valte, pred_valte_ttaug,\n",
    "                                                  ax1, ax2, ax3, ax4, ax5, \n",
    "                                                  perplexity=perp, max_iter=max_iter, \n",
    "                                                  variance_to_keep=variance_to_keep, k=num_neighbors, \n",
    "                                                  multicore_kNN=True, num_cores=multiprocessing.cpu_count(),\n",
    "                                                  plotting_order=plotting_order)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "del X_tr, y_tr, X_valte, y_valte, pred_valte, pred_tr, pred_tr_ttaug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ALIGNED t-SNE Maps with Uncertainty\n",
    "from FItSNE.fast_tsne import fast_tsne \n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import colors\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "\n",
    "def compute_uncertainties(predictions_1hot_ttaug, onset_level=1, mode='mean', use_entropy=False):\n",
    "    # Given MxTxC predictions, estimate the uncertainties w.r.t. the given mode: median, mean, or entropty of these.\n",
    "#     labels_bin = np.greater_equal(np.argmax(labels_1hot, axis=1), onset_level)\n",
    "    predictions_all_bin = np.sum(predictions_1hot_ttaug[:, :, onset_level:], axis=-1) # MxT\n",
    "    \n",
    "    uncertainty_est = None\n",
    "    predictions_bin_score = None\n",
    "    if mode == 'mean':\n",
    "        if use_entropy: # Entropy for mean predictions\n",
    "            predictions_bin_score = np.mean(predictions_all_bin, axis=1, keepdims=True) # Mx1\n",
    "            softmax_mean = np.concatenate((np.subtract(1.,predictions_bin_score), predictions_bin_score), axis=1) # Mx2\n",
    "            uncertainty_est = entropy(softmax_mean) # ENT from Mx2 matrix\n",
    "        else: # STD \n",
    "            uncertainty_est = np.std(predictions_all_bin, axis=1) # STD from MxT matrix\n",
    "        \n",
    "    elif mode == 'median':\n",
    "        if use_entropy:# Entropy for median predictions\n",
    "            predictions_bin_score = np.median(predictions_all_bin, axis=1, keepdims=True) # Mx1\n",
    "            softmax_median = np.concatenate((np.subtract(1.,predictions_bin_score), predictions_bin_score), axis=1) # Mx2\n",
    "            uncertainty_est = entropy(softmax_median) # ENT from Mx2 matrix\n",
    "        else: # IQR\n",
    "            uncertainty_est = stats.iqr(predictions_all_bin, axis=1) # IQR from MxT matrix\n",
    "    \n",
    "    assert uncertainty_est is not None, 'No uncertainty estimate computed!'\n",
    "    \n",
    "    return uncertainty_est\n",
    "\n",
    "def plot_given_map_with_uncertainty(Z, y, ax, title, point_size=2, markerscale=5, plotting_order='original', \n",
    "                                    uncertainty=None):\n",
    "    col = np.array(['burlywood','maroon', 'lightskyblue', 'darkslateblue', 'fuchsia'])\n",
    "    legend_labels = np.array(['0: No DR', '1: Mild DR', '2: Moderate DR', '3: Severe DR', '4: Proliferative DR'])\n",
    "    \n",
    "    if plotting_order == 'original':\n",
    "        plotting_order = range(len(np.unique(y)))\n",
    "    else:\n",
    "        plotting_order = [0,2,3,4,1] #[4,3,0,2,1]\n",
    "    \n",
    "    ax.set_facecolor('black')\n",
    "    \n",
    "    for i in plotting_order: # for i in range(len(np.unique(y))):\n",
    "#         print('Plotting for Class %g' % i)\n",
    "        mask = (y == i)\n",
    "        if uncertainty is not None:\n",
    "            col_rgb = []\n",
    "            col_names = col[y[mask]]\n",
    "            for k in range(len(col_names)):\n",
    "                col_rgb.append(colors.hex2color(colors.cnames[col_names[k]]))\n",
    "            col_rgb = np.asarray(col_rgb)\n",
    "#             c = colors.to_rgba_array(c=col_rgb, alpha=uncertainty[mask]) # np.subtract(1.0, uncertainty[mask])) \n",
    "            alpha = np.reshape(uncertainty[mask], newshape=(len(uncertainty[mask]), 1))\n",
    "            c = np.concatenate([col_rgb, alpha], axis=-1)\n",
    "            c = np.minimum(c, 1.0 - 1e-10)\n",
    "            c = np.maximum(c, 0.0 + 1e-10)\n",
    "        else:\n",
    "            c = col[y[mask]]\n",
    "        ax.scatter(Z[mask,0], Z[mask,1], c=c, label=legend_labels[i], s=point_size)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "#     leg = ax.legend(bbox_to_anchor=(0., 1.1325, 1., 0.), loc='upper center', ncol=3, \n",
    "    leg = ax.legend(bbox_to_anchor=(0., -0.05, 1., 0.), loc='upper center', ncol=3, \n",
    "                    mode=\"expand\", shadow=True, fancybox=True, markerscale=markerscale, handletextpad=0.1)\n",
    "    leg.get_frame().set_alpha(0.75)\n",
    "\n",
    "\n",
    "def plot_given_map_with_surface(Z, y, ax, title, uncertainty=None):\n",
    "        \n",
    "#     ax.set_facecolor('white')    \n",
    "    # Counter plot w.r.t. uncertainty\n",
    "#     XX, YY = np.meshgrid(Z[:,0], Z[:,1])\n",
    "#     ZZ, _ = np.meshgrid(uncertainty, uncertainty)\n",
    "#     CS = ax.contour(XX, YY, ZZ, colors='chartreuse')\n",
    "#     ax.clabel(CS, inline=True) #, fontsize=10)\n",
    "#     sns.kdeplot(data=Z[:,0], data2=Z[:,1], ax=ax)\n",
    "\n",
    "    im = ax.plot_trisurf(np.squeeze(Z[:,0]), np.squeeze(Z[:,1]), np.squeeze(uncertainty), cmap='viridis')\n",
    "    f.colorbar(im, ax=ax, shrink=0.8)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.view_init(azim=-90, elev=90)\n",
    "# #     leg = ax.legend(bbox_to_anchor=(0., 1.1325, 1., 0.), loc='upper center', ncol=3, \n",
    "#     leg = ax.legend(bbox_to_anchor=(0., -0.05, 1., 0.), loc='upper center', ncol=3, \n",
    "#                     mode=\"expand\", shadow=True, fancybox=True, markerscale=markerscale, handletextpad=0.1)\n",
    "#     leg.get_frame().set_alpha(0.75)\n",
    "\n",
    "\n",
    "\n",
    "def plot_aligned_tsne_with_uncertainty(Xa, ya, y_pred_a_ttaug,\n",
    "                                       Xb, yb, y_pred_b_ttaug, \n",
    "                                       axes, \n",
    "                                       perplexity=30, max_iter=1000, \n",
    "                                       variance_to_keep=0.99, k=10,\n",
    "                                       multicore_kNN=False, num_cores=10,\n",
    "                                       plotting_order='original',\n",
    "                                       exclude_reference_data=False,\n",
    "                                       use_entropy=False):\n",
    "        \n",
    "    # First, standardize the data\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    Xa = scaler.fit_transform(Xa)\n",
    "    Xb = scaler.transform(Xb)\n",
    "    \n",
    "    print('Computing the pairwise distances')\n",
    "    K = pairwise_distances(X=Xa, Y=Xb, metric='euclidean')\n",
    "    Ma, Mb = K.shape\n",
    "    print('Finding kNNs...')\n",
    "    kNN_idx_list = []\n",
    "    if not multicore_kNN:\n",
    "        for j in range(Mb): # loop over the items to be aligned with the reference map from Xa.\n",
    "            idx = np.argsort(K[:,j]) # ascending order, so most distant at the end. kNNs are in the front\n",
    "            kNN_idx_list.append(idx[:k]) # append the kNN indices\n",
    "    else:\n",
    "        kNNs_by_idx = Parallel(n_jobs=num_cores)(delayed(find_kNN_idx_per_column)(K[:,j], k) for j in range(Mb))\n",
    "        for j in range(len(kNNs_by_idx)):\n",
    "            kNN_idx_list.append(kNNs_by_idx[j])\n",
    "           \n",
    "    # Do PCA on the reference data and keep D dimensions\n",
    "    print('PCA on reference data ...')\n",
    "    Sigma = np.cov(np.transpose(Xa))\n",
    "    U, s, V = np.linalg.svd(Sigma, full_matrices=False)\n",
    "    sum_s = np.sum(s)\n",
    "    print('Total components : %g' % len(s))\n",
    "    for d in range(len(s)):\n",
    "        var_explained = np.sum(s[:d]) / sum_s\n",
    "        if var_explained >= variance_to_keep:\n",
    "            break\n",
    "    print('%g of variance explained with %d components.' % (var_explained, d))\n",
    "    \n",
    "    D = d\n",
    "    XaD = np.dot(Xa, U[:,:D])   # np.dot(U, np.diag(s))[:,:D]\n",
    "    PCAinit = XaD[:,:2] / np.std(XaD[:,0]) * 0.0001\n",
    "    \n",
    "    print('Computing tSNE map for the reference data')\n",
    "    Za = fast_tsne(XaD, perplexity=perplexity,\n",
    "                   max_iter=max_iter, learning_rate=learning_rate,\n",
    "#                   stop_early_exag_iter=250, early_exag_coeff=12,\n",
    "#                   start_late_exag_iter=750, late_exag_coeff=4, \n",
    "                   initialization=PCAinit)\n",
    "    print('tSNE done...')\n",
    "    \n",
    "    ##################################################################################\n",
    "    XbD = np.dot(Xb, U[:,:D])    # np.dot(U, np.diag(s))[:,:D]\n",
    "    \n",
    "    print('Collecting initialization points based on kNNs')\n",
    "    kNN_init = []\n",
    "    if not multicore_kNN:\n",
    "        for kNN_idx in kNN_idx_list:\n",
    "            kNNs = Za[kNN_idx,:2]\n",
    "            kNN_init.append(np.mean(kNNs, axis=0))\n",
    "            kNN_init = np.reshape(kNN_init, newshape=(len(kNN_idx_list),2))\n",
    "    else:\n",
    "        kNN_init = Parallel(n_jobs=num_cores)(delayed(find_kNN_inits_per_column)(kNN_idx_list[j], Za, 2) for j in range(len(kNN_idx_list)))\n",
    "        kNN_init = np.reshape(kNN_init, newshape=(len(kNN_idx_list), k, 2))\n",
    "        kNN_init = np.mean(kNN_init, axis=1)\n",
    "#     kNN_init = kNN_init[:,:2] / np.std(kNN_init[:,0]) * 0.0001\n",
    "    \n",
    "    print('Computing tSNE map for the auxillary data')\n",
    "    Zb = fast_tsne(XbD, perplexity=perplexity,\n",
    "                   max_iter=max_iter, learning_rate=learning_rate,\n",
    "#                   stop_early_exag_iter=250, early_exag_coeff=12,\n",
    "#                   start_late_exag_iter=750, late_exag_coeff=4, \n",
    "                   initialization=kNN_init)\n",
    "    print('tSNE done...')\n",
    "    \n",
    "    # Decide on the mappings to plot\n",
    "    if not exclude_reference_data:\n",
    "        Z = np.concatenate([Za,Zb], axis=0)\n",
    "        y = np.concatenate([ya,yb], axis=0)\n",
    "    else:\n",
    "        Z = Zb\n",
    "        y = yb\n",
    "    \n",
    "    # This one is for reference with no uncertainty\n",
    "    plot_given_map_with_uncertainty(Z, y, axes[0], 'Aligned tSNE' + ' Perplexity : ' + str(perplexity),\n",
    "                   plotting_order=plotting_order, uncertainty=None)\n",
    "    \n",
    "    # Now, the uncertainty business\n",
    "    # Rescale uncertainties into [0,1] and use them as alpha channel in t-SNE maps\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    \n",
    "    onset_levels = [1,2,3,4]\n",
    "    step = 1\n",
    "    for onset_level in onset_levels:\n",
    "#         print('Uncertainty w.r.t the onset level %g' % onset_level)\n",
    "        uncertainty_a = compute_uncertainties(y_pred_a_ttaug, onset_level=onset_level, \n",
    "                                              mode=mode, use_entropy=use_entropy)\n",
    "        uncertainty_b = compute_uncertainties(y_pred_b_ttaug, onset_level=onset_level, \n",
    "                                              mode=mode, use_entropy=use_entropy)\n",
    "        if not exclude_reference_data:\n",
    "            uncertainty = np.concatenate([uncertainty_a, uncertainty_b], axis=0)\n",
    "        else:\n",
    "            uncertainty = uncertainty_b\n",
    "        \n",
    "        uncertainty = np.reshape(uncertainty, newshape=(len(uncertainty),1))\n",
    "        uncertainty_01 = np.asarray(np.squeeze(min_max_scaler.fit_transform(uncertainty)), dtype=np.float32)\n",
    "        plot_given_map_with_uncertainty(Z, y, axes[onset_level+step-1], \n",
    "                                        'Aligned tSNE with Onset ' + str(onset_level)  + ' UNCERTAINTY,' + ' Perplexity : ' + str(perplexity),\n",
    "                                        plotting_order=plotting_order, uncertainty=uncertainty_01)\n",
    "        \n",
    "        plot_given_map_with_surface(Z, y, axes[onset_level+step+1-1], \n",
    "                                    'Uncertainty surface for onset ' + str(onset_level)  + ' Perplexity : ' + str(perplexity),\n",
    "                                    uncertainty=uncertainty)\n",
    "        \n",
    "        step += 1\n",
    "\n",
    "\n",
    "# Now, read the SINGLE PRED. results from file and plot\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_SINGpred.pkl'\n",
    "\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    result = pickle.load(filehandler)\n",
    "    \n",
    "    X_tr = result['train_features'] \n",
    "    X_val = result['val_features']\n",
    "    X_te = result['test_features']\n",
    "    \n",
    "    y_tr = np.argmax(result['train_labels_1hot'], axis=1)\n",
    "    y_val = np.argmax(result['val_labels_1hot'], axis=1)\n",
    "    y_te = np.argmax(result['test_labels_1hot'], axis=1)\n",
    "\n",
    "X_valte = np.concatenate([X_val, X_te], axis=0)\n",
    "y_valte = np.concatenate([y_val, y_te], axis=0)\n",
    "del result, X_val, y_val, X_te, y_te # make some room\n",
    "\n",
    "# Get the uncertainties from TTAUG results\n",
    "use_entropy = True\n",
    "exclude_reference_data=True\n",
    "\n",
    "# for T in T_values:\n",
    "print('T = %g' % T)\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '.pkl'\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    result_ttaug = pickle.load(filehandler)\n",
    "    \n",
    "#     labels_1hot_tr = result_ttaug['train_labels_1hot'] # Mx5\n",
    "#     labels_1hot_val = result_ttaug['val_labels_1hot']\n",
    "#     labels_1hot_te = result_ttaug['test_labels_1hot']\n",
    "    predictions_1hot_tr_ttaug = result_ttaug['train_pred_1hot'] # MxTx5\n",
    "    predictions_1hot_val_ttaug = result_ttaug['val_pred_1hot']\n",
    "    predictions_1hot_te_ttaug = result_ttaug['test_pred_1hot']\n",
    "\n",
    "predictions_1hot_valte_ttaug = np.concatenate([predictions_1hot_val_ttaug, predictions_1hot_te_ttaug], axis=0)\n",
    "\n",
    "\n",
    "for perp in perplexities:\n",
    "    print('Perplexity : %g' % perp)\n",
    "    f = plt.figure(figsize=(15, 37.5))\n",
    "    ######################################\n",
    "    ax1 = f.add_subplot(5, 2, 1) \n",
    "#     ax2 = f.add_subplot(5, 2, 2) \n",
    "    #####################################\n",
    "    # uncertainty maps for given onset levels \n",
    "    ax3 = f.add_subplot(5, 2, 3)\n",
    "    ax5 = f.add_subplot(5, 2, 5)\n",
    "    ax7 = f.add_subplot(5, 2, 7)\n",
    "    ax9 = f.add_subplot(5, 2, 9)\n",
    "    ######################################\n",
    "    # contour/surface plots\n",
    "    ax4 = f.add_subplot(5, 2, 4, projection='3d')\n",
    "    ax6 = f.add_subplot(5, 2, 6, projection='3d')\n",
    "    ax8 = f.add_subplot(5, 2, 8, projection='3d')\n",
    "    ax10 = f.add_subplot(5, 2, 10, projection='3d')\n",
    "    plot_aligned_tsne_with_uncertainty(X_tr, y_tr, predictions_1hot_tr_ttaug, \n",
    "                                       X_valte, y_valte, predictions_1hot_valte_ttaug, \n",
    "                                       [ax1, ax3, ax4, ax5, ax6, ax7, ax8, ax9, ax10], \n",
    "                                       perplexity=perp, max_iter=max_iter, \n",
    "                                       variance_to_keep=variance_to_keep, k=num_neighbors, \n",
    "                                       multicore_kNN=True, num_cores=multiprocessing.cpu_count(),\n",
    "                                       plotting_order=plotting_order, \n",
    "                                       exclude_reference_data=exclude_reference_data, \n",
    "                                       use_entropy=use_entropy)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "del X_tr, y_tr, X_valte, y_valte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ALIGNED t-SNE Maps with Uncertainty, MULTICLASS\n",
    "from FItSNE.fast_tsne import fast_tsne \n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import colors\n",
    "\n",
    "# plt.style.use('dark_background')\n",
    "\n",
    "def compute_uncertainties_multiclass(predictions_1hot_ttaug, mode='mean', use_entropy=False):\n",
    "    \n",
    "    uncertainty_est = None\n",
    "\n",
    "    if mode == 'mean':\n",
    "        if use_entropy: # Entropy for mean predictions\n",
    "            predictions_1hot_mean = np.mean(predictions_1hot_ttaug, axis=1, keepdims=False) # Mx5\n",
    "            predictions_1hot_mean = np.divide(predictions_1hot_mean, \n",
    "                                              np.sum(predictions_1hot_mean, axis=-1, \n",
    "                                                     keepdims=True)) # re-normalize the probabilities\n",
    "            uncertainty_est = entropy(predictions_1hot_mean) # ENT from Mx5 matrix\n",
    "        else: # STD \n",
    "            uncertainty_est = np.std(predictions_1hot_ttaug, axis=1) # STD from MxTx5 matrix\n",
    "        \n",
    "    elif mode == 'median':\n",
    "        if use_entropy:# Entropy for median predictions\n",
    "            predictions_1hot_median = np.median(predictions_1hot_ttaug, axis=1, keepdims=False) # Mx5\n",
    "            predictions_1hot_median = np.divide(predictions_1hot_median, \n",
    "                                                np.sum(predictions_1hot_median, axis=-1, \n",
    "                                                       keepdims=True)) # re-normalize the probabilities\n",
    "            uncertainty_est = entropy(predictions_1hot_median) # ENT from Mx5 matrix\n",
    "        else: # IQR\n",
    "            uncertainty_est = stats.iqr(predictions_1hot_ttaug, axis=1) # IQR from MxTx5 matrix\n",
    "    \n",
    "    assert uncertainty_est is not None, 'No uncertainty estimate computed!'\n",
    "    \n",
    "    return uncertainty_est\n",
    "\n",
    "\n",
    "def plot_aligned_tsne_with_uncertainty_multiclass(Xa, ya, y_pred_a_ttaug,\n",
    "                                                  Xb, yb, y_pred_b_ttaug, \n",
    "                                                  ax1, ax2, ax3, \n",
    "                                                  perplexity=30, max_iter=1000, \n",
    "                                                  variance_to_keep=0.99, k=10,\n",
    "                                                  multicore_kNN=False, num_cores=10,\n",
    "                                                  plotting_order='original',\n",
    "                                                  exclude_reference_data=False, use_entropy=False):\n",
    "    # First, standardize the data\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    Xa = scaler.fit_transform(Xa)\n",
    "    Xb = scaler.transform(Xb)\n",
    "    \n",
    "    print('Computing the pairwise distances')\n",
    "    K = pairwise_distances(X=Xa, Y=Xb, metric='euclidean')\n",
    "    Ma, Mb = K.shape\n",
    "    print('Finding kNNs...')\n",
    "    kNN_idx_list = []\n",
    "    if not multicore_kNN:\n",
    "        for j in range(Mb): # loop over the items to be aligned with the reference map from Xa.\n",
    "            idx = np.argsort(K[:,j]) # ascending order, so most distant at the end. kNNs are in the front\n",
    "            kNN_idx_list.append(idx[:k]) # append the kNN indices\n",
    "    else:\n",
    "        kNNs_by_idx = Parallel(n_jobs=num_cores)(delayed(find_kNN_idx_per_column)(K[:,j], k) for j in range(Mb))\n",
    "        for j in range(len(kNNs_by_idx)):\n",
    "            kNN_idx_list.append(kNNs_by_idx[j])\n",
    "           \n",
    "    # Do PCA on the reference data and keep D dimensions\n",
    "    print('PCA on reference data ...')\n",
    "    Sigma = np.cov(np.transpose(Xa))\n",
    "    U, s, V = np.linalg.svd(Sigma, full_matrices=False)\n",
    "    sum_s = np.sum(s)\n",
    "    print('Total components : %g' % len(s))\n",
    "    for d in range(len(s)):\n",
    "        var_explained = np.sum(s[:d]) / sum_s\n",
    "        if var_explained >= variance_to_keep:\n",
    "            break\n",
    "    print('%g of variance explained with %d components.' % (var_explained, d))\n",
    "    \n",
    "    D = d\n",
    "    XaD = np.dot(Xa, U[:,:D])   # np.dot(U, np.diag(s))[:,:D]\n",
    "    PCAinit = XaD[:,:2] / np.std(XaD[:,0]) * 0.0001\n",
    "    \n",
    "    print('Computing tSNE map for the reference data')\n",
    "    Za = fast_tsne(XaD, perplexity=perplexity,\n",
    "                   max_iter=max_iter, learning_rate=learning_rate,\n",
    "#                   stop_early_exag_iter=250, early_exag_coeff=12,\n",
    "#                   start_late_exag_iter=750, late_exag_coeff=4, \n",
    "                   initialization=PCAinit)\n",
    "    print('tSNE done...')\n",
    "    \n",
    "    ##################################################################################\n",
    "    XbD = np.dot(Xb, U[:,:D])    # np.dot(U, np.diag(s))[:,:D]\n",
    "    \n",
    "    print('Collecting initialization points based on kNNs')\n",
    "    kNN_init = []\n",
    "    if not multicore_kNN:\n",
    "        for kNN_idx in kNN_idx_list:\n",
    "            kNNs = Za[kNN_idx,:2]\n",
    "            kNN_init.append(np.mean(kNNs, axis=0))\n",
    "            kNN_init = np.reshape(kNN_init, newshape=(len(kNN_idx_list),2))\n",
    "    else:\n",
    "        kNN_init = Parallel(n_jobs=num_cores)(delayed(find_kNN_inits_per_column)(kNN_idx_list[j], Za, 2) for j in range(len(kNN_idx_list)))\n",
    "        kNN_init = np.reshape(kNN_init, newshape=(len(kNN_idx_list), k, 2))\n",
    "        kNN_init = np.mean(kNN_init, axis=1)\n",
    "#     kNN_init = kNN_init[:,:2] / np.std(kNN_init[:,0]) * 0.0001\n",
    "    \n",
    "    print('Computing tSNE map for the auxillary data')\n",
    "    Zb = fast_tsne(XbD, perplexity=perplexity,\n",
    "                   max_iter=max_iter, learning_rate=learning_rate, \n",
    "#                   stop_early_exag_iter=250, early_exag_coeff=12,\n",
    "#                   start_late_exag_iter=750, late_exag_coeff=4, \n",
    "                   initialization=kNN_init)\n",
    "    print('tSNE done...')\n",
    "    \n",
    "    # Decide on the mappings to plot\n",
    "    if not exclude_reference_data:\n",
    "        Z = np.concatenate([Za,Zb], axis=0)\n",
    "        y = np.concatenate([ya,yb], axis=0)\n",
    "    else:\n",
    "        Z = Zb\n",
    "        y = yb\n",
    "    \n",
    "    # This one is for reference with no uncertainty\n",
    "    plot_given_map_with_uncertainty(Z, y, ax1,                                    \n",
    "                                    'Aligned tSNE' + ' Perplexity : ' + str(perplexity),\n",
    "                                    plotting_order=plotting_order, uncertainty=None)\n",
    "    \n",
    "    # Now, the uncertainty business\n",
    "    # Rescale uncertainties into [0,1] and use them as alpha channel in t-SNE maps\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    \n",
    "    uncertainty_a = compute_uncertainties_multiclass(y_pred_a_ttaug, mode=mode, use_entropy=use_entropy)\n",
    "    uncertainty_b = compute_uncertainties_multiclass(y_pred_b_ttaug, mode=mode, use_entropy=use_entropy)\n",
    "    \n",
    "    if not exclude_reference_data:\n",
    "        uncertainty = np.concatenate([uncertainty_a, uncertainty_b], axis=0)\n",
    "    else:\n",
    "        uncertainty = uncertainty_b\n",
    "\n",
    "    uncertainty = np.reshape(uncertainty, newshape=(len(uncertainty),1))\n",
    "    uncertainty_01 = np.asarray(np.squeeze(min_max_scaler.fit_transform(uncertainty)), dtype=np.float32)\n",
    "    plot_given_map_with_uncertainty(Z, y, ax2, \n",
    "                                    'Aligned tSNE with Multi Class UNCERTAINTY, Perplexity : ' + str(perplexity),\n",
    "                                    plotting_order=plotting_order, uncertainty=uncertainty_01)\n",
    "    plot_given_map_with_surface(Z, y, ax3, \n",
    "                                'Multi Class Uncertainty surface, Perplexity : ' + str(perplexity),\n",
    "                                uncertainty=uncertainty)\n",
    "\n",
    "# Now, read the SINGLE PRED. results from file and plot\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_SINGpred.pkl'\n",
    "\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    result = pickle.load(filehandler)\n",
    "    \n",
    "    X_tr = result['train_features'] \n",
    "    X_val = result['val_features']\n",
    "    X_te = result['test_features']\n",
    "    \n",
    "    y_tr = np.argmax(result['train_labels_1hot'], axis=1)\n",
    "    y_val = np.argmax(result['val_labels_1hot'], axis=1)\n",
    "    y_te = np.argmax(result['test_labels_1hot'], axis=1)\n",
    "\n",
    "X_valte = np.concatenate([X_val, X_te], axis=0)\n",
    "y_valte = np.concatenate([y_val, y_te], axis=0)\n",
    "del result, X_val, y_val, X_te, y_te # make some room\n",
    "\n",
    "\n",
    "# for T in T_values:\n",
    "print('T = %g' % T)\n",
    "result_file_name = RESULTS_DIR + model.descriptor + '_TTAUG_' + str(T) + '.pkl'\n",
    "with open(result_file_name, 'rb') as filehandler:\n",
    "    result_ttaug = pickle.load(filehandler)\n",
    "    \n",
    "#     labels_1hot_tr = result_ttaug['train_labels_1hot'] # Mx5\n",
    "#     labels_1hot_val = result_ttaug['val_labels_1hot']\n",
    "#     labels_1hot_te = result_ttaug['test_labels_1hot']\n",
    "    predictions_1hot_tr_ttaug = result_ttaug['train_pred_1hot'] # MxTx5\n",
    "    predictions_1hot_val_ttaug = result_ttaug['val_pred_1hot']\n",
    "    predictions_1hot_te_ttaug = result_ttaug['test_pred_1hot']\n",
    "\n",
    "predictions_1hot_valte_ttaug = np.concatenate([predictions_1hot_val_ttaug, predictions_1hot_te_ttaug], axis=0)\n",
    "\n",
    "\n",
    "for perp in perplexities:\n",
    "    print('Perplexity : %g' % perp)\n",
    "    f = plt.figure(figsize=(22.5,7.5))\n",
    "    ax1 = f.add_subplot(1, 3, 1) \n",
    "    ax2 = f.add_subplot(1, 3, 2) \n",
    "    ax3 = f.add_subplot(1, 3, 3, projection='3d') \n",
    "    \n",
    "    plot_aligned_tsne_with_uncertainty_multiclass(X_tr, y_tr, predictions_1hot_tr_ttaug, \n",
    "                                                  X_valte, y_valte, predictions_1hot_valte_ttaug, \n",
    "                                                  ax1, ax2, ax3, \n",
    "                                                  perplexity=perp, max_iter=max_iter, \n",
    "                                                  variance_to_keep=variance_to_keep, k=num_neighbors, \n",
    "                                                  multicore_kNN=True, num_cores=multiprocessing.cpu_count(),\n",
    "                                                  plotting_order=plotting_order, \n",
    "                                                  exclude_reference_data=exclude_reference_data,\n",
    "                                                  use_entropy=use_entropy)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "del X_tr, y_tr, X_valte, y_valte"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
